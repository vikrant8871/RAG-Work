{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65e8fc7",
   "metadata": {},
   "source": [
    "# RAG model \n",
    "### PDF-DATA ingestion to vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2630be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7156f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: Statistic_For_Data_Science_1704226429.pdf\n",
      "  ✓ Loaded 38 pages\n",
      "\n",
      "Total documents loaded: 38\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669c56c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 0, 'page_label': '1', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 1 \\n \\nSH                                                       Statistics for Data Science \\n \\n \\nStatistics For Data Science \\n \\nWhat is Data? \\n According to the Oxford “Data is distinct pieces of information, usually \\nformatted in a special way”. \\nData is measured, collected and reported, and analysed, whereupon it is often \\nvisualized using graphs, images or other analysis tools. Raw data may be a collection of \\nnumbers or characters before it’s been cleaned and corrected by researchers. \\n \\nWhat is data science?  \\n Data science is the study of data It involves developing methods of recording, \\nstoring, and analysing data to effectively extract useful information. The goal of data \\nscience is to gain insights and knowledge from any type of data - both structured and \\nunstructured. \\n Its simple definition is Collect the information, make an analysis and take the \\nDecision for the growth of the business. \\n  \\nWhy Data Scientist? \\nData scientists straddle the world of both business and IT and possess unique \\nskill sets. Their role has assumed significance thanks to how businesses today think of \\nbig data. Business wants to make use of the unstructured data which can boost their \\nrevenue. Data scientists analyse this information to make sense of it and bring out \\nbusiness insights that will aid in the growth of the business. \\n \\n-------------------------------------------------------------------------------------------------------------------------------------------- \\n \\nAnalytics Project Life Cycle: \\n \\nThe following are the broadly divided steps which are usually performed in any \\nPredictive analytics / Machine Learning problem. \\n \\n1. Understanding the problem \\n2. Collecting and Reading data \\n3. Exploratory Data Analysis \\n4. Data cleaning/Data pre-processing \\n5. Data transformation \\n6. Data partition \\n7. Selecting few models \\n8. Cross validation for all chosen models \\n9. Evaluation of all models and selecting the best model \\n10. Predictions on unknown or unseen data'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 1, 'page_label': '2', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content=\"Page 2 \\n \\nSH                                                       Statistics for Data Science \\n \\n1. Understanding the Problem:  \\nBefore you solve a problem, you have to define the problem. \\nYou'll often get ambiguous inputs from the people who are your clients. You'll have to \\ndevelop the intuition to translate scare inputs in to actionable outputs and to ask the \\nquestions that nobody else is asking. \\nIf you are solving a problem for a VP of sales in a company. You should start \\nunderstanding by their goals and you need to work with them to clearly define the \\nproblem. \\n \\nTo define the problem, you need to ask the right questions. \\n1. Who are the customers? \\n2. Why are they buying our products? \\n3. How do we predict weather a customer is going to buy our product or not? \\n4. What is different from segments who are performing well and those that are \\nperforming below expectations? \\n5. How much money we may loss if we don’t actively sell the products to these groups? \\n \\n2. Collecting & Reading data: \\nThe first step is to get the data. Once you get the data you need to read the data \\ninto Machine Learning tools like R, python and so on. One should check the format of \\nthe data before reading. There can be many formats of the data. Commonly used and \\neasiest format of data is csv format. Sometimes you need to do ETL (Extract, \\nTransform and Load). \\nSimplest way to begin Machine Learning is to get the data in csv format. Data \\ncan be collected from repositories which are available free and publicly. \\n \\n3. Exploratory Data Analysis: \\nBefore proceeding towards doing anything related to data, one should clearly and \\nprecisely know about the problem and the questions which are required to be answered \\nthrough Machine Learning. Only then one can be certain about the results which the \\nMachine Learning algorithm is going to give. In the csv format, data is in the form of \\ntables which have rows and columns. One row belongs to one observation or record & \\none column belongs to one variable. Variable can be independent or dependent \\nvariable. So, one of the columns belongs to dependent variable, also known as target \\nvariable. One should check the meanings of each of the variables before going ahead. \\nOne should explore the data. There are many ways to explore the data including \\ndata visualization. This will help you get more insights on the problem and also it will \\nhelp you to get intuition on how you can get better results from Machine Learning. It \\ncan tell you which variables are important and it can also tell you which data columns \\nor rows have missing values. One can also find the patterns, if any in the data. \\n \\n4. Data Cleaning / Data pre-processing: \\nWe need to find the missing values like NA, NAN, blanks, etc and then impute \\n(or fill) them with something like average of non-missing values in the columns. We\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 2, 'page_label': '3', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 3 \\n \\nSH                                                       Statistics for Data Science \\n \\nalso need to remove the unnecessary columns and/or rows. One should do data \\nexploration before doing any data cleaning blindly. \\n \\n5. Data transformation: \\nFor numerical data we may require cantering, scaling or normalization like log-\\nnormalization, etc in order to avoid issues like overfitting. We may also require \\ndimensionality reduction techniques like Principal component analysis to remove \\ndimensionality issues. We may require one-hot encoding if we have categorical data. \\n \\n6. Data partition: \\nWe need to split the dataset into a training (known) set and testing (unknown) \\ndataset. We need “test data set” in order to validate the model or check the model \\nperformance on unseen or test dataset. \\n \\n7. Selecting few models: \\nBased on your intuition and your experience you can choose few models from \\nlist of machine learning models. They may or may not work so you need to choose \\ndifferent model then or you may need to tune the model. There are several models for \\ndifferent needs. For classification problems we have models like logistic regression, \\ndecision tree, random forest, etc and for regression problems we have models like \\nlinear regression, Gradient Descent, neural networks, lasso, etc. \\n \\n8. Cross validation for all chosen models: \\nModel is fitted on training or known data. One must do the cross-validation & \\nmodel tuning before making any conclusions about the results. Cross-validation is done \\nto issues like over fitting and model tuning is done to get the best model parameters \\nwhich can give best required results. Once you have chosen the models, then you can \\nperform model tuning and cross-validation for each of the chosen models. Cross-\\nvalidation is like repeatedly checking the model performance on unknown dataset and \\nthereby increasing the assurance of the model performance on any data set which will \\nbe fed into this model in future.  \\n \\n9. Evaluation of all models and selecting the best model: \\nOnce the model is fitted on the training data, it is used to predict the target or \\ndependent variable for the test data. The predicted value of the target is then compared \\nwith the actual target values of the test data set. The accuracy of the model is the \\npercentage of correct predictions which are made. There are several evaluation metrics \\nlike R2, RMSE, Accuracy score, F1 score, AUC, log loss and so on. Depending on the \\nrequirement you can choose evaluation metric and then calculate it for each of the \\nmodels. \\nThen you choose the model which has performed best in the evaluation. With \\nthis chosen model, you can then train this model on the training data set again. \\n \\n10.  Predictions on new data by deployment: \\nAnd now you ready to get the final predictions for the data which is unseen data.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 3, 'page_label': '4', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content=\"Page 4 \\n \\nSH                                                       Statistics for Data Science \\n \\nTypes of Data: \\nGenerally, data can be classified into two parts: \\n \\n1. Categorical Data: \\nCategorical data refers to a data type that can be stored and identified based on \\nthe names or labels given to them. A process called matching is done, to draw out the \\nsimilarities or relations between the data and then they are grouped accordingly.  \\nThe data collected in the categorical form is also known as qualitative data. Each \\ndataset can be grouped and labelled depending on their matching qualities, under only \\none category \\nExample: Marital Status, Political Party, Eye colour \\n \\n2. Numerical Data: \\nNumerical data can further be classified into two categories: \\n \\nDiscrete Data: \\nDiscrete data can take only discrete values. Discrete information contains only a \\nfinite number of possible values. Those values cannot be subdivided meaningfully. \\nHere, things can be counted in whole numbers. \\nWe speak of discrete data if the data can only take on certain values. This type of \\ndata can’t be measured but it can be counted. It basically represents information that \\ncan be categorized into a classification.  \\nExample: Number of students in the class, Mobiles, Brothers, etc. \\n \\nContinuous Data: \\nContinuous data is data that can be calculated. It has an infinite number of \\nprobable values that can be selected within a given specific range. \\nContinuous Data represents measurements and therefore their values can’t be \\ncounted but they can be measured. \\nExample: Temperature, Time, Age, Water, Currency, etc. \\n \\nAt advanced level, we can further classify the data into four parts \\n \\n1. Nominal Data: \\n Nominal data are used to label variables where there is no quantitative value and \\nhas no order. So, if you change the order of the value then the meaning will remain the \\nsame.  \\n Thus, nominal data are observed but not measured, are unordered but non-\\nequidistant, and have no meaningful zero.  \\nThe only numerical activities you can perform on nominal data is to state that \\nperception is (or isn't) equivalent to another (equity or inequity), and you can use this \\ndata to amass them. You can't organize nominal data, so you can't sort them. \\nNeither would you be able to do any numerical tasks as they are saved for \\nnumerical data. With nominal data, you can calculate frequencies, proportions, \\npercentages, and central points.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 4, 'page_label': '5', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content=\"Page 5 \\n \\nSH                                                       Statistics for Data Science \\n \\nExamples: Language, Gender, Location, Education, Yes/No, etc. \\n \\n2. Ordinal Data: \\nOrdinal data is almost the same as nominal data but not in the case of order as \\ntheir categories can be ordered like 1st, 2nd, etc. However, there is no continuity in the \\nrelative distances between adjacent categories. \\nOrdinal Data is observed but not measured, is ordered but non-equidistant, and \\nhas no meaningful zero. Ordinal scales are always used for measuring happiness, \\nsatisfaction, etc. \\nAs ordinal data are ordered, they can be arranged by making basic comparisons \\nbetween the categories, for example, greater or less than, higher or lower, and so on. \\nYou can't do any numerical activities with ordinal data, however, as they are numerical \\ndata. Examples: Excellent to poor, Grades, Opinion, etc. \\n \\n3. Interval Data: \\nInterval values represent ordered units that have the same difference. \\nTherefore, we speak of interval data when we have a variable that contains numeric \\nvalues that are ordered and where we know the exact differences between the values. \\nThe problem with interval values data is that they don’t have a true zero. That \\nmeans in regards to our example, that there is no such thing as no temperature. With \\ninterval data, we can add and subtract, but we cannot multiply, divide or calculate ratios. \\nBecause there is no true zero, a lot of descriptive and inferential statistics can’t be \\napplied. Example: Body temperature. \\n \\n4. Ratio Data: \\nRatio values are also ordered units that have the same difference. Ratio values \\nare the same as interval values, with the difference that they do have an absolute \\nzero. Examples: height, weight, length etc. \\n \\n5. Special Numeric Data: \\nPhone number, Pin codes, Aadhar number, Emp id, Transaction number, Order \\nnumber.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 5, 'page_label': '6', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 6 \\n \\nSH                                                       Statistics for Data Science \\n \\nStatistics: \\nStatistics is a branch of mathematics that deals with the study of collecting, \\nanalysing, interpreting, presenting, and organizing data in a particular manner. Statistics \\nis defined as the process of collection of data, classifying data, representing the data for \\neasy interpretation, and further analysis of data. Statistics also is referred to as arriving \\nat conclusions from the sample data that is collected using surveys or experiments. \\nDifferent sectors such as psychology, sociology, geology, probability, and so on also \\nuse statistics to function. \\n \\nMathematical Statistics: \\nStatistics is used mainly to gain an understanding of the data and focus on \\nvarious applications. Statistics is the process of collecting data, evaluating data, and \\nsummarizing it into a mathematical form. Initially, statistics were related to the science \\nof the state where it was used in the collection and analysis of facts and data about a \\ncountry such as its economy, population, etc. Mathematical statistics applies \\nmathematical techniques like linear algebra, differential equations, mathematical \\nanalysis, and theories of probability. \\nThere are two methods of analysing data in mathematical statistics that are used \\non a large scale: \\n \\nDescriptive Statistics: \\nThe descriptive method of statistics is used to describe the data collected and \\nsummarize the data and its properties using the measures of central tendencies and the \\nmeasures of dispersion. \\n \\nInferential Statistics: \\nThis method of statistics is used to draw conclusions from the data. Inferential \\nstatistics requires statistical tests performed on samples, and it draws conclusions by \\nidentifying the differences between the 2 groups. Tests calculate the p-value that is \\ncompared with the probability of chance(α) = 0.05. If the p-value is less than α, then it \\nis concluded that the p-value is statistically significant. \\n \\nDescriptive Statistics Inferential Statistics \\nDescriptive statistics are used to quantify \\nthe characteristics of the data. \\nInferential statistics are used to make \\nconclusions about the population by using \\nanalytical tools on the sample data \\nMeasures of central tendency and \\nmeasures of dispersion are the important \\ntools used. \\nHypothesis testing and regression analysis \\nare the analytical tools used. \\nIt is used to describe the characteristics of \\na known sample or population. \\nIt is used to make inferences about an \\nunknown population \\nMeasures of descriptive statistics are \\nvariance, range, mean, median, etc. \\nMeasures of inferential statistics are t-test, \\nz test, linear regression, etc.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 6, 'page_label': '7', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 7 \\n \\nSH                                                       Statistics for Data Science \\n \\nDescriptive Statistics:  \\nThe study of numerical and graphical ways to describe and display your data is \\ncalled descriptive statistics. It describes the data and helps us understand the features of \\nthe data by summarizing the given sample set or population of data. In descriptive \\nstatistics, we usually take the sample into account. \\nStatisticians use graphical representation of data to get a clear picture of the data. \\nBusiness trends can be analysed easily with these representations. visual representation \\nis more effective than presenting huge numbers. \\nWe can describe these data in various dimensions. Various dimensions of \\ndescribing data are \\n \\n1. Central Tendency of Data \\n2. Dispersion of Data \\n3. Shape of the Data \\n \\n1. Central Tendency of Data: \\nThis is the centre of the distribution of data. It describes the location of data and \\nconcentrates where the data is located. \\nThe three most widely used measures of the “centre” of the data are \\n \\n1.1 Mean \\nThe “Mean” is the average of the data. \\nAverage can be identified by summing up all the numbers and then dividing \\nthem by the number of observations. \\nMean = X1 + X2 + X3 +… + Xn / n \\nExample: Data – 10,20,30,40,50 and Number of observations = 5 \\nMean = [ 10+20+30+40+50] / 5 \\nMean = 30 \\nOutliers influence the central tendency of the data. \\n \\n1.2 Median \\nMedian is the 50%th percentile of the data. It is exactly the centre point of the \\ndata. \\nMedian can be identified by ordering the data and splits the data into two equal \\nparts and find the number. It is the best way to find the centre of the data. \\nBecause the central tendency of the data is not affected by outliers. Outliers don’t \\ninfluence the data. \\nExample: Odd number of Data – 10,20,30,40,50 \\nMedian is 30. \\nEven number of data – 10,20,30,40,50,60 \\nFind the middle 2 data and take the mean of those two values. \\nHere 30 and 40 are middle values. \\n= 30+40 / 2  \\n= 35.  Median is 35'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 7, 'page_label': '8', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 8 \\n \\nSH                                                       Statistics for Data Science \\n \\n1.3 Mode \\nMode is frequently occurring data or elements. \\nIf an element occurs the highest number of times, it is the mode of that data. If no \\nnumber in the data is repeated, then there is no mode for that data. There can be more \\nthan one mode in a dataset if two values have the same frequency and also the highest \\nfrequency. \\n \\nOutliers don’t influence the data. \\nThe mode can be calculated for both quantitative and qualitative data. \\nExample: Data – 1,3,4,6,7,3,3,5,10, 3 \\nMode is 3 \\nbecause 3 has the highest frequency (4 times) \\n \\n2. Dispersion of Data: \\n \\n  \\nThe dispersion is the “Spread of the data”. It measures how far the data is spread. \\nIn most of the dataset, the data values are closely located near the mean. On some other \\ndataset, the values are widely spread out of the mean. These dispersions of data can be \\nmeasured by  \\n \\n2.1 Inter Quartile Range (IQR) \\nQuartiles are special percentiles. \\n1st Quartile Q1 is the same as the 25th percentile. \\n2nd Quartile Q2 is the same as 50th percentile. \\n3rd Quartile Q3 is same as 75th percentile'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 8, 'page_label': '9', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 9 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nSteps to find quartile and percentile: \\n–The data should sort and ordered from the smallest to the largest. \\n–For Quartiles, ordered data is divided into 4 equal parts. \\n–For Percentiles, ordered data is divided into 100 equal parts. \\nInter Quartile Range is the difference between the third quartile(Q3) and the first \\nQuartile (Q1) \\nIQR = Q3- Q1 \\nInter Quartile range \\nIt is the spread of the middle half (50%) of the data \\n \\n2.2 Range \\nThe range is the difference between the largest and the smallest value in the data. \\nMax – Min = Range \\n \\n2.3 Standard Deviation \\nThe most common measure of spread is the standard deviation. \\nThe Standard deviation is the measure of how far the data deviates from the \\nmean value. \\nThe standard deviation formula varies for population and sample. Both formulas \\nare similar, but not the same. \\n• Symbol used for Sample Standard Deviation – “s” (lowercase) \\n• Symbol used for Population Standard Deviation – “σ” (sigma, lower case) \\n \\nSteps to find Standard deviation: \\nIf x is a number, then the difference “x – mean” is its deviation. The deviations \\nare used to calculate the standard deviation. \\nSample Standard Deviation, s = Square root of sample variance  \\nSample Standard Deviation, s = Square root of [Σ (x − x ¯ )2/ n-1]   where x ¯ is \\naverage and n is no. of samples'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 9, 'page_label': '10', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 10 \\n \\nSH                                                       Statistics for Data Science \\n \\n Standard Deviation for sample \\n \\nPopulation Standard Deviation, σ = Square root of population variance \\nPopulation Standard Deviation, σ = Square root of [ Σ (x − μ)2 / N] where μ is \\nMean and N is no. of population. \\n \\n The standard deviation for population \\n \\nThe standard deviation is always positive or zero. It will be large when the data \\nvalues are spread out from the mean. \\n \\n2.4 Variance \\nThe variance is a measure of variability. It is the average squared deviation from \\nthe mean. \\nThe symbol σ2 represents the population variance and the symbol for s2 \\nrepresents sample variance.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 10, 'page_label': '11', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 11 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\n3. Shape of the Data: \\nThe shape describes the type of the graph. \\nThe shape of the data is important because making a decision about the \\nprobability of data is based on its shape. \\nThe shape of the data can be measured by two methodologies. \\n \\n3.1 Symmetric: \\nIn the symmetric shape of the graph, the data is distributed the same on both \\nsides. \\nIn symmetric data, the mean and median are located close together. \\n \\n The curve formed by this symmetric graph is called a normal curve. \\n \\n3.2 Skewness: \\nSkewness is the measure of the asymmetry of the distribution of data. \\nThe data is not symmetrical (i.e.) it is skewed towards one side. \\nSkewness is classified into two types.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 11, 'page_label': '12', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 12 \\n \\nSH                                                       Statistics for Data Science \\n \\n1. Positively skewed \\nIn a Positively skewed distribution, the data values are clustered around the left \\nside of the distribution and the right side is longer. \\nThe mean and median will be greater than the mode in the positive skew. \\n \\n2. Negatively skewed \\nIn a Negatively skewed distribution, the data values are clustered around the \\nright side of the distribution and the left side is longer. \\nThe mean and median will be less than the mode in the negative skew. \\n \\n  \\n3.3 Kurtosis \\nKurtosis is the measure of describing the distribution of data. \\nThis data is distributed in different ways. They are, \\n \\n1. Platykurtic \\nThe platykurtic shows a distribution with flat tails. Here the data is distributed \\nflatly. The flat tails indicated the small outliers in the distribution. \\n \\n2. Mesokurtic \\nIn Mesokurtic, the data is widely distributed. It is normally distributed and it also \\nmatches normal distribution. \\n \\n 3. Leptokurtic \\nIn leptokurtic, the data is very closely distributed. The height of the peak is \\ngreater than width of the peak.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 12, 'page_label': '13', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 13 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nScatter Plot: \\nA scatter plot uses dots to represent values for two different numeric variables. \\nThe position of each dot on the horizontal and vertical axis indicates values for an \\nindividual data point. Scatter plots are used to observe relationships between variables.  \\nScatter plots’ primary uses are to observe and show relationships between two \\nnumeric variables. The dots in a scatter plot not only report the values of individual \\ndata points, but also patterns when the data are taken as a whole. \\n \\nScatter plot Correlation: \\nWe know that the correlation is a statistical measure of the relationship between \\nthe two variables’ relative movements. If the variables are correlated, the points will \\nfall along a line or curve. The better the correlation, the closer the points will touch the \\nline. This cause examination tool is considered as one of the seven essential quality \\ntools. \\n \\nTypes of correlation: \\nThe scatter plot explains the correlation between two attributes or variables. It \\nrepresents how closely the two variables are connected. There can be three such \\nsituations to see the relation between the two variables – \\n \\nPositive Correlation: \\nWhen the points in the graph are rising, moving from left to right, then the scatter \\nplot shows a positive correlation. It means the values of one variable are increasing \\nwith respect to another. \\n \\nNegative Correlation: \\nWhen the points in the scatter graph fall while moving left to right, then it is \\ncalled a negative correlation. It means the values of one variable are decreasing with \\nrespect to another.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 13, 'page_label': '14', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 14 \\n \\nSH                                                       Statistics for Data Science \\n \\nNo Correlation: \\nWhen the points are scattered all over the graph and it is difficult to conclude \\nwhether the values are increasing or decreasing, then there is no correlation between the \\nvariables.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 14, 'page_label': '15', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 15 \\n \\nSH                                                       Statistics for Data Science \\n \\nInferential Statistics: \\nInferential statistics is a branch of statistics that makes the use of various \\nanalytical tools to draw inferences about the population data from sample data. Apart \\nfrom inferential statistics, descriptive statistics forms another branch of statistics. \\nInferential statistics help to draw conclusions about the population while descriptive \\nstatistics summarizes the features of the data set. \\nThere are two main types of inferential statistics - hypothesis testing and \\nregression analysis. The samples chosen in inferential statistics need to be \\nrepresentative of the entire population. In this article, we will learn more about \\ninferential statistics, its types, examples, and see the important formulas. \\n \\nWhat are Inferential Statistics? \\nInferential statistics helps to develop a good understanding of the population data \\nby analysing the samples obtained from it. It helps in making generalizations about the \\npopulation by using various analytical tests and tools. In order to pick out random \\nsamples that will represent the population accurately many sampling techniques are \\nused. Some of the important methods are simple random sampling, stratified sampling, \\ncluster sampling, and systematic sampling techniques. \\n \\nInferential Statistics Definition: \\nInferential statistics can be defined as a field of statistics that uses analytical tools \\nfor drawing conclusions about a population by examining random samples. The goal of \\ninferential statistics is to make generalizations about a population. In inferential \\nstatistics, a statistic is taken from the sample data (e.g., the sample mean) that used to \\nmake inferences about the population parameter (e.g., the population mean). \\n \\nInferential Statistics Examples: \\nInferential statistics is very useful and cost-effective as it can make inferences \\nabout the population without collecting the complete data. Some inferential statistics \\nexamples are given below: \\n• Suppose the mean marks of 100 students in a particular country are known. \\nUsing this sample information, the mean marks of students in the country can be \\napproximated using inferential statistics. \\n• Suppose a coach wants to find out how many average cartwheels sophomores at \\nhis college can do without stopping. A sample of a few students will be asked to \\nperform cartwheels and the average will be calculated. Inferential statistics will use this \\ndata to make a conclusion regarding how many cartwheel sophomores can perform on \\naverage. \\n \\nWhy do we need Inferential Statistics? \\nIn contrast to Descriptive Statistics, rather than having access to the whole \\npopulation, we often have a limited amount of data. \\nIn such cases, Inferential Statistics come into action. For example, we might be \\ninterested in finding the average of the entire school’s exam marks. It is not reasonable \\nbecause we might find it impracticable to get the data we need. So, rather than getting'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 15, 'page_label': '16', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 16 \\n \\nSH                                                       Statistics for Data Science \\n \\nthe entire school’s exam marks, we measure a smaller sample of students (for example, \\na sample of 50 students). This sample of 50 students will now describe the complete \\npopulation of all students of that school. \\nSimply put, Inferential Statistics make predictions about a population based on a \\nsample of data taken from that population. \\n \\nThe technique of Inferential Statistics involves the following steps: \\n\\uf0b7 First, take some samples and try to find one that represents the entire \\npopulation accurately. \\n\\uf0b7 Next, test the sample and use it to draw generalizations about the whole \\npopulation. \\n \\nTypes of Inferential Statistics: \\n1. Estimating parameters:  \\nWe take a statistic from the collected data, such as the standard deviation, and \\nuse it to define a more general parameter, such as the standard deviation of the \\ncomplete population. \\n \\n2. Hypothesis testing:  \\nVery beneficial when we are looking to gather data on something that can only \\nbe given to a very confined population, such as a new drug. If we want to know \\nwhether this drug will work for all patients (“complete population”), we can use the \\ndata collected to predict this (often by calculating a z-score). \\n \\n3. Confidence Interval: \\nThe confidence interval is the range of values that you expect your estimate to \\nfall between a certain percentage of the time if you run your experiment again or re-\\nsample the population in the same way. \\n \\nThe confidence level is the percentage of times you expect to reproduce an \\nestimate between the upper and lower bounds of the confidence interval, and is set by \\nthe alpha value. \\n \\nConfidence interval for the mean of normally-distributed data: \\nNormally-distributed data forms a bell shape when plotted on a graph, with the \\nsample mean in the middle and the rest of the data distributed fairly evenly on either \\nside of the mean. \\nThe confidence interval for data which follows a standard normal distribution is:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 16, 'page_label': '17', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 17 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nWhere: \\n\\uf0b7 CI = the confidence interval \\n\\uf0b7 X̄  = the population mean \\n\\uf0b7 Z = the critical value of the z-distribution \\n\\uf0b7 σ = the population standard deviation \\n\\uf0b7 √n = the square root of the population size \\n \\nWhat exactly is a confidence interval? \\nA confidence interval is the mean of your estimate plus and minus the variation \\nin that estimate. This is the range of values you expect your estimate to fall between if \\nyou redo your test, within a certain level of confidence. \\n \\nConfidence, in statistics, is another way to describe probability. For example, if \\nyou construct a confidence interval with a 95% confidence level, you are confident that \\n95 out of 100 times the estimate will fall between the upper and lower values specified \\nby the confidence interval. \\n \\nYour desired confidence level is usually one minus the alpha (α) value you used \\nin your statistical test \\nConfidence level = 1 – α \\nSo, if you use an alpha value of p < 0.05 for statistical significance, then your \\nconfidence level would be 1 − 0.05 = 0.95, or 95%. \\n \\nWhen do you use confidence intervals? \\nYou can calculate confidence intervals for many kinds of statistical estimates, \\nincluding: \\n \\n\\uf0b7 Proportions \\n\\uf0b7 Population means \\n\\uf0b7 Differences between population means or proportions \\n\\uf0b7 Estimates of variation among groups \\nThese are all point estimates, and don’t give any information about the variation \\naround the number. Confidence intervals are useful for communicating the variation \\naround a point estimate.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 17, 'page_label': '18', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 18 \\n \\nSH                                                       Statistics for Data Science \\n \\nExample: Variation around an estimate \\nYou survey 100 Brits and 100 Americans about their television-watching habits, \\nand find that both groups watch an average of 35 hours of television per week. \\n \\nHowever, the British people surveyed had a wide variation in the number of \\nhours watched, while the Americans all watched similar amounts. \\n \\nEven though both groups have the same point estimate (average number of hours \\nwatched), the British estimate will have a wider confidence interval than the American \\nestimate because there is more variation in the data. \\n \\nFinding the standard deviation: \\nMost statistical software will have a built-in function to calculate your standard \\ndeviation, but to find it by hand you can first find your sample variance, then take the \\nsquare root to get the standard deviation. \\n \\n1. Find the sample variance \\nSample variance is defined as the sum of squared differences from the mean, also \\nknown as the mean-squared-error (MSE): \\n \\n  \\nTo find the MSE, subtract your sample mean from each value in the dataset, square \\nthe resulting number, and divide that number by n − 1 \\nThen add up all of these numbers to get your total sample variance (s2). For larger \\nsample sets, it’s easiest to do this in Excel. \\n \\n2. Find the standard deviation. \\nThe standard deviation of your estimate (s) is equal to the square root of the sample \\nvariance/sample error (s2):'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 18, 'page_label': '19', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 19 \\n \\nSH                                                       Statistics for Data Science \\n \\nExample: \\nIn the television-watching survey, the variance in the GB estimate is 100, while the \\nvariance in the USA estimate is 25. Taking the square root of the variance gives us a \\nsample standard deviation (s) of: \\n\\uf0b7 10 for the GB estimate. \\n\\uf0b7 5 for the USA estimate. \\n \\nCentral Limit Theorem: \\nThe central limit theorem is the basis for how normal distributions work in \\nstatistics. \\nIn research, to get a good idea of a population mean, ideally you’d collect data \\nfrom multiple random samples within the population. A sampling distribution of the \\nmean is the distribution of the means of these different samples. \\nThe central limit theorem shows the following: \\n\\uf0b7 Law of Large Numbers: As you increase sample size (or the number of \\nsamples), then the sample mean will approach the population mean. \\n\\uf0b7 With multiple large samples, the sampling distribution of the mean is \\nnormally distributed, even if your original variable is not normally \\ndistributed. \\nParametric statistical tests typically assume that samples come from normally \\ndistributed populations, but the central limit theorem means that this assumption isn’t \\nnecessary to meet when you have a large enough sample. \\nYou can use parametric tests for large samples from populations with any kind of \\ndistribution as long as other important assumptions are met. A sample size of 30 or \\nmore is generally considered large. \\nFor small samples, the assumption of normality is important because the \\nsampling distribution of the mean isn’t known. For accurate results, you have to be sure \\nthat the population is normally distributed before you can use parametric tests with \\nsmall samples. \\n \\nWhy Is the Central Limit Theorem Useful? \\nThe central limit theorem is useful when analysing large data sets because it allows \\none to assume that the sampling distribution of the mean will be normally-distributed in \\nmost cases. This allows for easier statistical analysis and inference.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 19, 'page_label': '20', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 20 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nHypothesis Testing: \\nHypothesis testing is a part of statistics in which we make assumptions about the \\npopulation parameter. So, hypothesis testing mentions a proper procedure by analysing \\na random sample of the population to accept or reject the assumption. \\nHypothesis testing is the way of trying to make sense of assumptions by looking \\nat the sample data. \\n \\nType of Hypothesis: \\nThe best way to determine whether a statistical hypothesis is true would be to \\nexamine the entire population. Since that is often impractical, researchers typically \\nexamine a random sample from the population. If sample data are not consistent with \\nthe statistical hypothesis, the hypothesis is rejected. \\nThere are two types of statistical hypotheses. \\n\\uf0b7 Null Hypothesis. The null hypothesis, denoted by Ho, is usually the \\nhypothesis that sample observations result purely from chance. \\n\\uf0b7 Alternative Hypothesis. The alternative hypothesis, denoted by H1 or Ha, is \\nthe hypothesis that sample observations are influenced by some non-random \\ncause. \\n \\nSteps of Hypothesis Testing: \\nThe process to determine whether to reject a null hypothesis or to fail to reject the \\nnull hypothesis, based on sample data is called hypothesis testing. It consists of four \\nsteps: \\n1. Define the null and alternate hypothesis \\n2. Define an analysis plan to find how to use sample data to estimate the null \\nhypothesis \\n3. Do some analysis on the sample data to create a single number called ‘test \\nstatistic’ \\n4. Understand the result by applying the decision rule to check whether the Null \\nhypothesis is true or not'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 20, 'page_label': '21', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 21 \\n \\nSH                                                       Statistics for Data Science \\n \\nIf the value of t-stat is less than the significance level we will reject the null \\nhypothesis, otherwise, we will fail to reject the null hypothesis. \\nTechnically, we never accept the null hypothesis, we say that either we fail to reject \\nor we reject the null hypothesis. \\n \\nErrors in hypothesis testing: \\nWe have explained what is hypothesis testing and the steps to do the testing. \\nNow, while performing the hypothesis testing, there might be some errors. \\n\\uf0b7 Type I error. A Type I error occurs when the researcher rejects a null \\nhypothesis when it is true. The probability of committing a Type I error is \\ncalled the significance level. This probability is also called alpha and is often \\ndenoted by α. \\n\\uf0b7 Type II error. A Type II error occurs when the researcher fails to reject a \\nfalse null hypothesis. The probability of committing a Type II error is \\ncalled beta and is often denoted by β. The probability of not committing a \\nType II error is called the Power of the test. \\n \\nDecision \\n------------------------------ \\nActual \\nReject the null \\nhypothesis \\nFail to reject the null \\nhypothesis \\nNull Hypothesis is True Type-1 Error Decision is correct \\nAlternate hypothesis is \\ntrue \\nDecision is correct Type-2 Error \\n \\n \\nTerms in Hypothesis testing: \\nSignificance level: \\nThe significance level is defined as the probability of the case when we reject the \\nnull hypothesis, but in actuality, it is true. For example, a 0.05 significance level \\nindicates that there is a 5% risk in assuming that there is some difference when, in \\nactuality, there is no difference. It is denoted by alpha (α). \\nThe below figure shows that the two shaded regions are equidistant from the null \\nhypothesis, each having a probability of 0.025 and a total of 0.05, which is our \\nsignificance level. The shaded region in case of a two-tailed test is called the critical \\nregion.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 21, 'page_label': '22', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 22 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nP-value: \\nThe p-value is defined as the probability of seeing a t-statistic as extreme as the \\ncalculated value if the null hypothesis value is true. A low enough p-value is the ground \\nfor rejecting the null hypothesis. We reject the null hypothesis if the p-value is less than \\nthe significance level. \\n \\nZ-test: \\nA z test is used on data that follows a normal distribution and has a sample size \\ngreater than or equal to 30. It is used to test if the means of the sample and population \\nare equal when the population variance is known. The right tailed hypothesis can be set \\nup as follows: \\n \\nNull Hypothesis:  H0: μ=μ0 \\nAlternate Hypothesis:  H1: μ>μ0 \\nDecision Criteria: If the z statistic > z critical value then rejects the null hypothesis. \\nWe find the Z-statistic of the sample means and calculate the z-score. Z-score is \\ngiven by the formula,  \\n  \\nZ-test is mainly used when the population mean and standard deviation are \\ngiven. \\n \\nT-test: \\nA t test is used when the data follows a student t distribution and the sample size \\nis lesser than 30. It is used to compare the sample and population mean when the \\npopulation variance is unknown. The hypothesis test for inferential statistics is given as \\nfollows:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 22, 'page_label': '23', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 23 \\n \\nSH                                                       Statistics for Data Science \\n \\n \\nNull Hypothesis:   H0: μ=μ0 \\nAlternate Hypothesis: H1: μ>μ0 \\nDecision Criteria: If the t statistic > t critical value then rejects the null hypothesis. \\nThe Sample Standard Deviation is given as: \\n \\n  \\nwhere n-1 is Bessel’s correction for estimating the population parameter. \\nAnother difference between z-scores and t-values is that t-values are dependent \\non the Degree of Freedom of a sample. Let us define what degree of freedom is for a \\nsample. \\n \\nThe Degree of Freedom: \\nIt is the number of variables that have the choice of having more than one \\narbitrary value. For example, in a sample of size 10 with a mean of 10, 9 values can be \\narbitrary, but the 10th value is forced by the sample mean. \\nPoints to note about the t-tests: \\n1. The greater the difference between the sample mean and the population mean, \\nthe greater the chance of rejecting the Null Hypothesis. \\n2. Greater the sample size, the greater the chance of rejection of the Null \\nHypothesis. \\n \\nDifferent types of T-tests: \\n1. One Sample T-test: \\nThe one-sample t-test compares the mean of sample data to a known value. So, if \\nwe have to compare the mean of sample data to the population mean, we use the One-\\nSample T-test. \\nWe can run a one-sample T-test when we do not have the population S.D., or we \\nhave a sample of size less than 30. \\nt-statistic is given by: \\n  \\nwhere, X bar is the sample mean, μ the population mean, s the sample standard \\ndeviation, and N the sample size.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 23, 'page_label': '24', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 24 \\n \\nSH                                                       Statistics for Data Science \\n \\n2. Two sample T-test: \\nWe use a two-sample T-test when we want to evaluate whether the mean of the \\ntwo samples is different or not. In a two-sample T-test, we have another two categories: \\n \\n\\uf0b7 Independent Sample T-test: \\nIndependent sample means that the two different samples should be \\nselected from two completely different populations. In other words, we can say \\nthat one population should not be dependent on the other population. \\n \\n  \\n\\uf0b7 Paired T-test: \\nIf our samples are connected in some way, we have to use the paired t-test. \\nHere, ‘connecting’ means that the samples are connected as we are collecting \\ndata from the same group two times, e.g., blood tests of patients of a hospital \\nbefore and after medication. \\n \\n \\n  \\nChi-Square test: \\nThe Chi-square test is used in the case when we have to compare categorical \\ndata. \\nThe Chi-square test is of two types. Both use chi-square statistics and distribution \\nfor different purposes. \\n\\uf0b7 The goodness of fit: It determines if sample data of categorical variables \\nmatch with population or not. \\n\\uf0b7 Test of Independence: It compares two categorical variables to find whether \\nthey are related to each other or not.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 24, 'page_label': '25', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 25 \\n \\nSH                                                       Statistics for Data Science \\n \\nChi-square statistic is given by: \\n \\n  \\nANOVA (Analysis of variance): \\nANOVA (Analysis of Variance) is used to check if at least one of two or more \\ngroups have statistically different means. Now, the question arises — Why do we need \\nanother test for checking the difference of means between independent groups? Why \\ncan we not use multiple t-tests to check for the difference in means? \\nThe answer is simple. Multiple t-tests will have a compound effect on the error \\nrate of the result. Performing a t-test thrice will give an error rate of ~15%, which is too \\nhigh, whereas ANOVA keeps it at 5% for a 95% confidence interval. \\nTo perform an ANOVA, you must have a continuous response variable and at \\nleast one categorical factor with two or more levels. ANOVA requires data from \\napproximately normally distributed populations with equal variances between factor \\nlevels. \\nThere are two types of ANOVA test: \\n1. One-way ANOVA: when only 1 independent variable is considered. \\n2. Two-way ANOVA: when 2 independent variables are considered. \\n3. N-way ANOVA: when N number of independent variables are considered. \\n  \\nCorrelation Coefficient (R or r): \\nIt is used to measure the strength between two variables. It is simply the square \\nroot of the coefficient of Determination and ranges from -1 to 1 where 0 represents no \\ncorrelation, and 1 represents positive strong correlation while -1 represents negative \\nstrong correlation.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 25, 'page_label': '26', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 26 \\n \\nSH                                                       Statistics for Data Science \\n \\nDifference between Z-Test and T-Test: \\n \\nBasis Z Test T-Test \\nBasic \\nDefinition \\nZ-test is a kind of hypothesis test which \\nascertains if the averages of the 2 \\ndatasets are different from each other \\nwhen standard deviation or variance is \\ngiven. \\nThe t-test can be referred to as a kind of \\nparametric test that is applied to an \\nidentity, how the averages of 2 sets of data \\ndiffer from each other when the standard \\ndeviation or variance is not given. \\nPopulation \\nVariance \\nThe Population variance or standard \\ndeviation is known here. \\nThe Population variance or standard \\ndeviation is unknown here. \\nSample Size The Sample size is large. Here the Sample Size is small. \\nKey \\nAssumptions \\nAll data points are independent. \\nNormal Distribution for Z, with an \\naverage zero and variance = 1. \\nAll data points are not dependent. \\nSample values are to be recorded and \\ntaken accurately. \\nBased upon  \\n(a type of \\ndistribution) \\nBased on Normal distribution. Based on Student-t distribution. \\n \\nDifference between One Tailed Test and Two Tailed Test: \\n \\nBASIS OF \\nCOMPARISON ONE-TAILED TEST TWO-TAILED TEST \\nMeaning A statistical hypothesis test in which \\nalternative hypothesis has only one end, \\nis known as one tailed test. \\nA significance test in which \\nalternative hypothesis has two ends, \\nis called two-tailed test. \\nHypothesis Directional Non-directional \\nRegion of rejection Either left or right Both left and right \\nDetermines If there is a relationship between \\nvariables in single direction. \\nIf there is a relationship between \\nvariables in either direction. \\nResult Greater or less than certain value. Greater or less than certain range of \\nvalues. \\nSign in alternative \\nhypothesis \\n> or < ≠'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 26, 'page_label': '27', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 27 \\n \\nSH                                                       Statistics for Data Science \\n \\nSample Vs Population: \\nSample: \\nA sample is the specific group that you will collect data from. The size of the \\nsample is always less than the total size of the population. \\nYou should calculate the sample standard deviation when the dataset you’re \\nworking with represents a sample taken from a larger population of interest. The \\nformula to calculate a sample standard deviation, denoted as s. \\nWhatever statistical measures I can calculate Here we called them as \\n“Descriptive statistics.” \\n \\nPopulation: \\nA population is the entire group that you want to draw conclusions about. \\nYou should calculate the population standard deviation when the dataset you’re \\nworking with represents an entire population, i.e., every value that you’re interested in. \\nThe formula to calculate a population standard deviation, denoted as σ. \\nWhatever the sample data, I have I will apply some additional theory and I will \\nestimate on population. ‘Inferential statistics” \\n \\n  \\nΣ: A symbol that means “sum” \\nxi: The ith value in a dataset \\nx̄ : The sample mean \\nn: The sample size \\nΣ: A symbol that means “sum” \\nxi: The ith value in a dataset \\nμ: The population mean \\nN: The population size \\n \\nBrute Force method:  \\nWhen we try to get all the data and compute it to make a statement, i.e., when we \\nreach out to the whole population. But it is difficult to compute and get each and every \\nindividual, that why we are getting into Sample.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 27, 'page_label': '28', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 28 \\n \\nSH                                                       Statistics for Data Science \\n \\nReasons for sampling: \\n\\uf0b7 Necessity:  \\nSometimes it’s simply not possible to study the whole population due to its \\nsize or inaccessibility. \\n \\n\\uf0b7 Practicality: \\nIt’s easier and more efficient to collect data from a sample. \\n \\n\\uf0b7 Cost-effectiveness:  \\nThere are fewer participant, laboratory, equipment, and researcher costs \\ninvolved. \\n \\n\\uf0b7 Manageability: \\nStoring and running statistical analyses on smaller datasets is easier and \\nreliable. \\n \\nPractice Problem 1: Height \\nSuppose a gym teacher wants to summarize the mean and standard deviation of \\nheights of students in his class. \\nWhen calculating the standard deviation of height, should he use the population \\nor sample standard deviation formula? \\nAnswer: He should use the population standard deviation because he is only \\ninterested in the height of students in this one particular class. \\n \\nPractice Problem 2: Manufacturing \\nSuppose an inspector wants to summarize the mean and standard deviation of the \\nweight of tires produced at a certain factory. He decides to collect a simple random \\nsample of 40 tires from the factory and weighs each of them. \\nWhen calculating the standard deviation of weights, should he use the population \\nor sample standard deviation formula? \\nAnswer: He should use the sample standard deviation because he is interested \\nin the weights of all tires produced at this factory, not just the weights of the tires in his \\nsample. \\n \\nPractice Problem 3: Biology \\nSuppose a biologist wants to summarize the mean and standard deviation of the \\nweight of a particular species of turtles. She decides to go out and collect a simple \\nrandom sample of 20 turtles from the population. \\nWhen calculating the standard deviation of weights, should she use the \\npopulation or sample standard deviation formula? \\nAnswer: She should use the sample standard deviation because she is \\ninterested in the weights of the entire population of turtles, not just the weights of the \\nturtles in her sample.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 28, 'page_label': '29', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 29 \\n \\nSH                                                       Statistics for Data Science \\n \\nPopulation and Sampling methods: \\nThe population contains all the data points from a set of data, while a sample \\nconsists of few observations selected from the population. The sample from the \\npopulation should be selected such that it has all the properties that a population has. \\nPopulation’s measurable properties such as mean, standard deviation, etc., are \\ncalled parameters, while Sample’s measurable property is known as a statistic. \\nTo draw valid conclusions from your results, you have to carefully decide how you \\nwill select a sample that is representative of the group as a whole. There are two types \\nof sampling methods: \\n \\n\\uf0b7 Probability sampling involves random selection, allowing you to make strong \\nstatistical inferences about the whole group. \\n\\uf0b7 Non-probability sampling involves non-random selection based on convenience \\nor other criteria, allowing you to easily collect data. \\n \\nProbability Sampling: (Unbiased Sample) \\nProbability sampling means that every member of the population has a chance of \\nbeing selected. It is mainly used in quantitative research. If you want to produce results \\nthat are representative of the whole population, probability sampling techniques are the \\nmost valid choice. \\nThere are four main types of probability sample. \\n \\n  \\n1. Simple random sampling: \\nIn a simple random sample, every member of the population has an equal chance \\nof being selected. Your sampling frame should include the whole population. \\nTo conduct this type of sampling, you can use tools like random number \\ngenerators or other techniques that are based entirely on chance.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 29, 'page_label': '30', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 30 \\n \\nSH                                                       Statistics for Data Science \\n \\nExample: You want to select a simple random sample of 100 employees of \\nCompany X. You assign a number to every employee in the company database from 1 \\nto 1000, and use a random number generator to select 100 numbers. \\n \\n2. Systematic sampling: \\nSystematic sampling is similar to simple random sampling, but it is usually \\nslightly easier to conduct. Every member of the population is listed with a number, but \\ninstead of randomly generating numbers, individuals are chosen at regular intervals. \\nExample: All employees of the company are listed in alphabetical order. From the \\nfirst 10 numbers, you randomly select a starting point: number 6. From number 6 \\nonwards, every 10th person on the list is selected (6, 16, 26, 36, and so on), and you \\nend up with a sample of 100 people. \\nIf you use this technique, it is important to make sure that there is no hidden \\npattern in the list that might skew the sample. For example, if the HR database groups \\nemployees by team, and team members are listed in order of seniority, there is a risk \\nthat your interval might skip over people in junior roles, resulting in a sample that is \\nskewed towards senior employees. \\n \\n3. Stratified sampling: \\nStratified sampling involves dividing the population into subpopulations that may \\ndiffer in important ways. It allows you draw more precise conclusions by ensuring that \\nevery subgroup is properly represented in the sample. \\nTo use this sampling method, you divide the population into subgroups (called \\nstrata) based on the relevant characteristic (e.g., gender, age range, income bracket, job \\nrole). \\nBased on the overall proportions of the population, you calculate how many \\npeople should be sampled from each subgroup. Then you use random or systematic \\nsampling to select a sample from each subgroup. \\nExample: The company has 800 female employees and 200 male employees. You \\nwant to ensure that the sample reflects the gender balance of the company, so you sort \\nthe population into two strata based on gender. Then you use random sampling on each \\ngroup, selecting 80 women and 20 men, which gives you a representative sample of \\n100 people. \\n \\n4. Cluster sampling: \\nCluster sampling also involves dividing the population into subgroups, but each \\nsubgroup should have similar characteristics to the whole sample. Instead of sampling \\nindividuals from each subgroup, you randomly select entire subgroups. \\nIf it is practically possible, you might include every individual from each sampled \\ncluster. If the clusters themselves are large, you can also sample individuals from \\nwithin each cluster using one of the techniques above. This is called multistage \\nsampling. \\nThis method is good for dealing with large and dispersed populations, but there is \\nmore risk of error in the sample, as there could be substantial differences between'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 30, 'page_label': '31', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 31 \\n \\nSH                                                       Statistics for Data Science \\n \\nclusters. It’s difficult to guarantee that the sampled clusters are really representative of \\nthe whole population. \\nExample: The company has offices in 10 cities across the country (all with \\nroughly the same number of employees in similar roles). You don’t have the capacity to \\ntravel to every office to collect your data, so you use random sampling to select 3 \\noffices – these are your clusters. \\n \\nNon-probability sampling: (Biased Sample) \\nIn a non-probability sample, individuals are selected based on non-random \\ncriteria, and not every individual has a chance of being included. \\nThis type of sample is easier and cheaper to access, but it has a higher risk \\nof sampling bias. That means the inferences you can make about the population are \\nweaker than with probability samples, and your conclusions may be more limited. If \\nyou use a non-probability sample, you should still aim to make it as representative of \\nthe population as possible. \\nNon-probability sampling techniques are often used \\nin exploratory and qualitative research. In these types of research, the aim is not to test \\na hypothesis about a broad population, but to develop an initial understanding of a \\nsmall or under-researched population. \\n \\n  \\n1. Convenience sampling: \\nA convenience sample simply includes the individuals who happen to be most \\naccessible to the researcher. \\nThis is an easy and inexpensive way to gather initial data, but there is no way to \\ntell if the sample is representative of the population, so it can’t produce generalizable \\nresults. \\nExample: You are researching opinions about student support services in your \\nuniversity, so after each of your classes, you ask your fellow students to complete'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 31, 'page_label': '32', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 32 \\n \\nSH                                                       Statistics for Data Science \\n \\na survey on the topic. This is a convenient way to gather data, but as you only surveyed \\nstudents taking the same classes as you at the same level, the sample is not \\nrepresentative of all the students at your university. \\n \\n2. Voluntary response sampling: \\nSimilar to a convenience sample, a voluntary response sample is mainly based on \\nease of access. Instead of the researcher choosing participants and directly contacting \\nthem, people volunteer themselves (e.g., by responding to a public online survey). \\nVoluntary response samples are always at least somewhat biased, as some people \\nwill inherently be more likely to volunteer than others. \\nExample: You send out the survey to all students at your university and a lot of \\nstudents decide to complete it. This can certainly give you some insight into the topic, \\nbut the people who responded are more likely to be those who have strong opinions \\nabout the student support services, so you can’t be sure that their opinions are \\nrepresentative of all students. \\n \\n3. Purposive sampling: \\nThis type of sampling, also known as judgement sampling, involves the \\nresearcher using their expertise to select a sample that is most useful to the purposes of \\nthe research. \\nIt is often used in qualitative research, where the researcher wants to gain \\ndetailed knowledge about a specific phenomenon rather than make statistical \\ninferences, or where the population is very small and specific. An effective purposive \\nsample must have clear criteria and rationale for inclusion. \\nExample: You want to know more about the opinions and experiences of \\ndisabled students at your university, so you purposefully select a number of students \\nwith different support needs in order to gather a varied range of data on their \\nexperiences with student services. \\n \\n4. Snowball sampling \\nIf the population is hard to access, snowball sampling can be used to recruit \\nparticipants via other participants. The number of people you have access to \\n“snowballs” as you get in contact with more people. \\nExample: You are researching experiences of homelessness in your city. Since \\nthere is no list of all homeless people in the city, probability sampling isn’t possible. \\nYou meet one person who agrees to participate in the research, and she puts you in \\ncontact with other homeless people that she knows in the area.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 32, 'page_label': '33', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 33 \\n \\nSH                                                       Statistics for Data Science \\n \\nRandom Variable and Probability Distributions: \\nProbability is defined as the likeliness of something to occur or happen \\nand probability distributions are functions that give the relation between all the \\noutcomes of a random variable in any random experiment and its probable values. \\nThese distribution functions are used in predicting the stock prices, weather \\nprediction. \\n \\nWhat is Random Variable? \\nSet of all possible values from a Random Experiment is called Random Variable. \\nIt is represented by X. \\nExample: Outcome of coin toss. \\n  \\nTypes of Random Variable: \\n\\uf0b7 Discrete Random Variable: \\nX is a discrete because it has a countable value between two numbers \\nExample: number of balls in a bag, number of tails in tossing coin \\n\\uf0b7 Continuous Random Variable: \\nX is a continuous because it has an infinite number of values between two values \\nExample: distance travelled, Height of students \\n  \\nWhat is Probability Distribution? \\nA Probability Distribution of a random variable is a list of all possible outcomes \\nwith corresponding probability values. \\nNote: The value of the probability always lies between 0 to 1. \\n \\n  \\nWhat is an example of Probability Distribution? \\nWhen two dice are rolled with six sided dots, let the possible probability of \\nrolling is as follows:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 33, 'page_label': '34', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 34 \\n \\nSH                                                       Statistics for Data Science \\n \\n \\n\\uf0b7 If a random variable is a discrete variable, its probability distribution is called \\ndiscrete probability distribution. \\no Example: Flipping of two coins \\no Functions that represent a discrete probability distribution is known \\nas Probability Mass Function. \\n\\uf0b7 If a random variable is a continuous variable, its probability distribution is called \\ncontinuous probability distribution. \\no Example: Measuring temperature over a period of time \\no Functions that represent a continuous probability distribution is known \\nas Probability Density Function. \\n \\nTypes of Probability Distributions:   \\nUniform Distribution: \\nProbability distribution in which all the outcome has equal probability is known \\nas Uniform Distribution. \\nExample: Perfect Random Generator \\nConsider an experiment of tossing a single coin: \\n \\n  \\nBernoulli Distribution: \\nA discrete probability distribution for a random experiment that has only two \\npossible outcomes (Bernoulli trials) is known Bernoulli Distribution. \\nExample: India will win cricket world cup or not'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 34, 'page_label': '35', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 35 \\n \\nSH                                                       Statistics for Data Science \\n \\nIt has only two possible outcomes \\no Success (1) \\no Failure (0) \\nConsider an experiment of Shooting of Basketball \\n \\n  \\nBinomial Distribution: \\nA discrete probability distribution that gives only two possible outcomes in n \\nindependent trails is known as Binomial Distribution. \\nExample: Yes/No survey \\n\\uf0b7 Extension of Bernoulli Distribution \\n\\uf0b7 Represent the number of success and failure into n independent trials \\n\\uf0b7 The probability of success and failure is the same for all independent and \\nidentical trails. \\nLet’s understand the Binomial Distribution by an example, \\nConsider the experiment of Picking Balls \\nProblem Statement:  \\nLet there are 8 white balls and 2 black balls, then the probability of drawing 3 white \\nballs, if the probability of selecting white ball is 0.6.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 35, 'page_label': '36', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 36 \\n \\nSH                                                       Statistics for Data Science \\n \\nBernoulli Binomial \\nDeals with the single trial event Deals with the outcome of Multiple trials of the \\nsingle events \\nHas only two possible outcome 0 and 1 Sum of identically and independent distributed \\nBernoulli Random Variable \\n \\nPoisson Distribution: \\nA discrete probability distribution that measures the probability of a random \\nvariable over a specific period of time is known as Poisson Distribution. \\nExample: Probability of Asteroid collision over a selected year of period. \\n\\uf0b7 Used to predict probability of number of successful events. \\n\\uf0b7 Random variable X is Poisson distributed if the distribution function is given by: \\nNote: In case of Poisson Distribution Mean = Variance \\nLet’s understand the Poisson Distribution by an example, \\nConsider the experiment of Number of patients visiting in a hospital \\nProblem Statement: \\nLet in a hospital patient arriving in a hospital at expected value is 6, then what is the \\nprobability of five patients will visit the hospital in that day? \\n \\n  \\nPoisson Binomial \\nNumber of trials are infinite Number of trials are fixed \\nUnlimited number of possible outcomes Only two possible outcomes (Success or \\nFailure) \\nMean = Variance Mean > Variance'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 36, 'page_label': '37', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 37 \\n \\nSH                                                       Statistics for Data Science \\n \\nNormal Distribution: \\n(Gaussian Distribution, Bell Curve, Symmetric Around Mean): \\nA continuous probability distribution, which is symmetric about its mean value \\n(i.e., data near the mean are more frequency in occurrence) is known as Normal \\nDistribution. \\nLet’s understand the Normal Distribution by an example, \\nConsider the experiment of Number of books read by students in a school \\n \\n  \\nEmpirical Rule: \\nEmpirical Rule is often called the 68 – 95 – 99.7 rule or Three Sigma Rule. It states \\nthat on a Normal Distribution: \\n\\uf0b7 68% of the data will be within one Standard Deviation of the Mean \\n\\uf0b7 95% of the data will be within two Standard Deviations of the Mean \\n\\uf0b7 99.7 of the data will be within three Standard Deviations of the Mean'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 37, 'page_label': '38', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 38 \\n \\nSH                                                       Statistics for Data Science \\n \\nA Normal distribution curve has the following properties: \\no Symmetrical around its mean value \\no Mean = Median = Mode \\no Total area under the curve is 1 \\no Curve of the distribution is bell curve \\no The curve is symmetric, with half of the values on the left and half of the \\nvalues on the right. \\nDifference between Poisson and Normal Distribution: \\n \\nPoisson Normal \\nUse Discrete Data Use Continuous Data \\nDistribution varies on mean value Symmetric about mean value \\nMean = Variance Mean = Median = Mode \\n \\nStandard normal distribution: \\n\\uf0b7 Normal distribution with mean = 0 and standard deviation = 1.  \\n \\n  \\n \\nProbability distributions are not a Graph. \\nA graph is just a visual representation. \\n \\nThe End')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74cae812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text splitting into chunks\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09296e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 38 documents into 91 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Page 1 \n",
      " \n",
      "SH                                                       Statistics for Data Science \n",
      " \n",
      " \n",
      "Statistics For Data Science \n",
      " \n",
      "What is Data? \n",
      " According to the Oxford “Data is distinct pieces of i...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 0, 'page_label': '1', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 0, 'page_label': '1', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 1 \\n \\nSH                                                       Statistics for Data Science \\n \\n \\nStatistics For Data Science \\n \\nWhat is Data? \\n According to the Oxford “Data is distinct pieces of information, usually \\nformatted in a special way”. \\nData is measured, collected and reported, and analysed, whereupon it is often \\nvisualized using graphs, images or other analysis tools. Raw data may be a collection of \\nnumbers or characters before it’s been cleaned and corrected by researchers. \\n \\nWhat is data science?  \\n Data science is the study of data It involves developing methods of recording, \\nstoring, and analysing data to effectively extract useful information. The goal of data \\nscience is to gain insights and knowledge from any type of data - both structured and \\nunstructured. \\n Its simple definition is Collect the information, make an analysis and take the \\nDecision for the growth of the business. \\n  \\nWhy Data Scientist?'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 0, 'page_label': '1', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='unstructured. \\n Its simple definition is Collect the information, make an analysis and take the \\nDecision for the growth of the business. \\n  \\nWhy Data Scientist? \\nData scientists straddle the world of both business and IT and possess unique \\nskill sets. Their role has assumed significance thanks to how businesses today think of \\nbig data. Business wants to make use of the unstructured data which can boost their \\nrevenue. Data scientists analyse this information to make sense of it and bring out \\nbusiness insights that will aid in the growth of the business. \\n \\n-------------------------------------------------------------------------------------------------------------------------------------------- \\n \\nAnalytics Project Life Cycle: \\n \\nThe following are the broadly divided steps which are usually performed in any \\nPredictive analytics / Machine Learning problem. \\n \\n1. Understanding the problem \\n2. Collecting and Reading data \\n3. Exploratory Data Analysis'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 0, 'page_label': '1', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Predictive analytics / Machine Learning problem. \\n \\n1. Understanding the problem \\n2. Collecting and Reading data \\n3. Exploratory Data Analysis \\n4. Data cleaning/Data pre-processing \\n5. Data transformation \\n6. Data partition \\n7. Selecting few models \\n8. Cross validation for all chosen models \\n9. Evaluation of all models and selecting the best model \\n10. Predictions on unknown or unseen data'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 1, 'page_label': '2', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content=\"Page 2 \\n \\nSH                                                       Statistics for Data Science \\n \\n1. Understanding the Problem:  \\nBefore you solve a problem, you have to define the problem. \\nYou'll often get ambiguous inputs from the people who are your clients. You'll have to \\ndevelop the intuition to translate scare inputs in to actionable outputs and to ask the \\nquestions that nobody else is asking. \\nIf you are solving a problem for a VP of sales in a company. You should start \\nunderstanding by their goals and you need to work with them to clearly define the \\nproblem. \\n \\nTo define the problem, you need to ask the right questions. \\n1. Who are the customers? \\n2. Why are they buying our products? \\n3. How do we predict weather a customer is going to buy our product or not? \\n4. What is different from segments who are performing well and those that are \\nperforming below expectations? \\n5. How much money we may loss if we don’t actively sell the products to these groups?\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 1, 'page_label': '2', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='4. What is different from segments who are performing well and those that are \\nperforming below expectations? \\n5. How much money we may loss if we don’t actively sell the products to these groups? \\n \\n2. Collecting & Reading data: \\nThe first step is to get the data. Once you get the data you need to read the data \\ninto Machine Learning tools like R, python and so on. One should check the format of \\nthe data before reading. There can be many formats of the data. Commonly used and \\neasiest format of data is csv format. Sometimes you need to do ETL (Extract, \\nTransform and Load). \\nSimplest way to begin Machine Learning is to get the data in csv format. Data \\ncan be collected from repositories which are available free and publicly. \\n \\n3. Exploratory Data Analysis: \\nBefore proceeding towards doing anything related to data, one should clearly and \\nprecisely know about the problem and the questions which are required to be answered'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 1, 'page_label': '2', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='3. Exploratory Data Analysis: \\nBefore proceeding towards doing anything related to data, one should clearly and \\nprecisely know about the problem and the questions which are required to be answered \\nthrough Machine Learning. Only then one can be certain about the results which the \\nMachine Learning algorithm is going to give. In the csv format, data is in the form of \\ntables which have rows and columns. One row belongs to one observation or record & \\none column belongs to one variable. Variable can be independent or dependent \\nvariable. So, one of the columns belongs to dependent variable, also known as target \\nvariable. One should check the meanings of each of the variables before going ahead. \\nOne should explore the data. There are many ways to explore the data including \\ndata visualization. This will help you get more insights on the problem and also it will \\nhelp you to get intuition on how you can get better results from Machine Learning. It'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 1, 'page_label': '2', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='data visualization. This will help you get more insights on the problem and also it will \\nhelp you to get intuition on how you can get better results from Machine Learning. It \\ncan tell you which variables are important and it can also tell you which data columns \\nor rows have missing values. One can also find the patterns, if any in the data. \\n \\n4. Data Cleaning / Data pre-processing: \\nWe need to find the missing values like NA, NAN, blanks, etc and then impute \\n(or fill) them with something like average of non-missing values in the columns. We'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 2, 'page_label': '3', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 3 \\n \\nSH                                                       Statistics for Data Science \\n \\nalso need to remove the unnecessary columns and/or rows. One should do data \\nexploration before doing any data cleaning blindly. \\n \\n5. Data transformation: \\nFor numerical data we may require cantering, scaling or normalization like log-\\nnormalization, etc in order to avoid issues like overfitting. We may also require \\ndimensionality reduction techniques like Principal component analysis to remove \\ndimensionality issues. We may require one-hot encoding if we have categorical data. \\n \\n6. Data partition: \\nWe need to split the dataset into a training (known) set and testing (unknown) \\ndataset. We need “test data set” in order to validate the model or check the model \\nperformance on unseen or test dataset. \\n \\n7. Selecting few models: \\nBased on your intuition and your experience you can choose few models from \\nlist of machine learning models. They may or may not work so you need to choose'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 2, 'page_label': '3', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='7. Selecting few models: \\nBased on your intuition and your experience you can choose few models from \\nlist of machine learning models. They may or may not work so you need to choose \\ndifferent model then or you may need to tune the model. There are several models for \\ndifferent needs. For classification problems we have models like logistic regression, \\ndecision tree, random forest, etc and for regression problems we have models like \\nlinear regression, Gradient Descent, neural networks, lasso, etc. \\n \\n8. Cross validation for all chosen models: \\nModel is fitted on training or known data. One must do the cross-validation & \\nmodel tuning before making any conclusions about the results. Cross-validation is done \\nto issues like over fitting and model tuning is done to get the best model parameters \\nwhich can give best required results. Once you have chosen the models, then you can \\nperform model tuning and cross-validation for each of the chosen models. Cross-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 2, 'page_label': '3', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='which can give best required results. Once you have chosen the models, then you can \\nperform model tuning and cross-validation for each of the chosen models. Cross-\\nvalidation is like repeatedly checking the model performance on unknown dataset and \\nthereby increasing the assurance of the model performance on any data set which will \\nbe fed into this model in future.  \\n \\n9. Evaluation of all models and selecting the best model: \\nOnce the model is fitted on the training data, it is used to predict the target or \\ndependent variable for the test data. The predicted value of the target is then compared \\nwith the actual target values of the test data set. The accuracy of the model is the \\npercentage of correct predictions which are made. There are several evaluation metrics \\nlike R2, RMSE, Accuracy score, F1 score, AUC, log loss and so on. Depending on the \\nrequirement you can choose evaluation metric and then calculate it for each of the \\nmodels.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 2, 'page_label': '3', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='like R2, RMSE, Accuracy score, F1 score, AUC, log loss and so on. Depending on the \\nrequirement you can choose evaluation metric and then calculate it for each of the \\nmodels. \\nThen you choose the model which has performed best in the evaluation. With \\nthis chosen model, you can then train this model on the training data set again. \\n \\n10.  Predictions on new data by deployment: \\nAnd now you ready to get the final predictions for the data which is unseen data.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 3, 'page_label': '4', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 4 \\n \\nSH                                                       Statistics for Data Science \\n \\nTypes of Data: \\nGenerally, data can be classified into two parts: \\n \\n1. Categorical Data: \\nCategorical data refers to a data type that can be stored and identified based on \\nthe names or labels given to them. A process called matching is done, to draw out the \\nsimilarities or relations between the data and then they are grouped accordingly.  \\nThe data collected in the categorical form is also known as qualitative data. Each \\ndataset can be grouped and labelled depending on their matching qualities, under only \\none category \\nExample: Marital Status, Political Party, Eye colour \\n \\n2. Numerical Data: \\nNumerical data can further be classified into two categories: \\n \\nDiscrete Data: \\nDiscrete data can take only discrete values. Discrete information contains only a \\nfinite number of possible values. Those values cannot be subdivided meaningfully. \\nHere, things can be counted in whole numbers.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 3, 'page_label': '4', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='finite number of possible values. Those values cannot be subdivided meaningfully. \\nHere, things can be counted in whole numbers. \\nWe speak of discrete data if the data can only take on certain values. This type of \\ndata can’t be measured but it can be counted. It basically represents information that \\ncan be categorized into a classification.  \\nExample: Number of students in the class, Mobiles, Brothers, etc. \\n \\nContinuous Data: \\nContinuous data is data that can be calculated. It has an infinite number of \\nprobable values that can be selected within a given specific range. \\nContinuous Data represents measurements and therefore their values can’t be \\ncounted but they can be measured. \\nExample: Temperature, Time, Age, Water, Currency, etc. \\n \\nAt advanced level, we can further classify the data into four parts \\n \\n1. Nominal Data: \\n Nominal data are used to label variables where there is no quantitative value and'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 3, 'page_label': '4', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content=\"At advanced level, we can further classify the data into four parts \\n \\n1. Nominal Data: \\n Nominal data are used to label variables where there is no quantitative value and \\nhas no order. So, if you change the order of the value then the meaning will remain the \\nsame.  \\n Thus, nominal data are observed but not measured, are unordered but non-\\nequidistant, and have no meaningful zero.  \\nThe only numerical activities you can perform on nominal data is to state that \\nperception is (or isn't) equivalent to another (equity or inequity), and you can use this \\ndata to amass them. You can't organize nominal data, so you can't sort them. \\nNeither would you be able to do any numerical tasks as they are saved for \\nnumerical data. With nominal data, you can calculate frequencies, proportions, \\npercentages, and central points.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 4, 'page_label': '5', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content=\"Page 5 \\n \\nSH                                                       Statistics for Data Science \\n \\nExamples: Language, Gender, Location, Education, Yes/No, etc. \\n \\n2. Ordinal Data: \\nOrdinal data is almost the same as nominal data but not in the case of order as \\ntheir categories can be ordered like 1st, 2nd, etc. However, there is no continuity in the \\nrelative distances between adjacent categories. \\nOrdinal Data is observed but not measured, is ordered but non-equidistant, and \\nhas no meaningful zero. Ordinal scales are always used for measuring happiness, \\nsatisfaction, etc. \\nAs ordinal data are ordered, they can be arranged by making basic comparisons \\nbetween the categories, for example, greater or less than, higher or lower, and so on. \\nYou can't do any numerical activities with ordinal data, however, as they are numerical \\ndata. Examples: Excellent to poor, Grades, Opinion, etc. \\n \\n3. Interval Data: \\nInterval values represent ordered units that have the same difference.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 4, 'page_label': '5', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='data. Examples: Excellent to poor, Grades, Opinion, etc. \\n \\n3. Interval Data: \\nInterval values represent ordered units that have the same difference. \\nTherefore, we speak of interval data when we have a variable that contains numeric \\nvalues that are ordered and where we know the exact differences between the values. \\nThe problem with interval values data is that they don’t have a true zero. That \\nmeans in regards to our example, that there is no such thing as no temperature. With \\ninterval data, we can add and subtract, but we cannot multiply, divide or calculate ratios. \\nBecause there is no true zero, a lot of descriptive and inferential statistics can’t be \\napplied. Example: Body temperature. \\n \\n4. Ratio Data: \\nRatio values are also ordered units that have the same difference. Ratio values \\nare the same as interval values, with the difference that they do have an absolute \\nzero. Examples: height, weight, length etc. \\n \\n5. Special Numeric Data:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 4, 'page_label': '5', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='are the same as interval values, with the difference that they do have an absolute \\nzero. Examples: height, weight, length etc. \\n \\n5. Special Numeric Data: \\nPhone number, Pin codes, Aadhar number, Emp id, Transaction number, Order \\nnumber.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 5, 'page_label': '6', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 6 \\n \\nSH                                                       Statistics for Data Science \\n \\nStatistics: \\nStatistics is a branch of mathematics that deals with the study of collecting, \\nanalysing, interpreting, presenting, and organizing data in a particular manner. Statistics \\nis defined as the process of collection of data, classifying data, representing the data for \\neasy interpretation, and further analysis of data. Statistics also is referred to as arriving \\nat conclusions from the sample data that is collected using surveys or experiments. \\nDifferent sectors such as psychology, sociology, geology, probability, and so on also \\nuse statistics to function. \\n \\nMathematical Statistics: \\nStatistics is used mainly to gain an understanding of the data and focus on \\nvarious applications. Statistics is the process of collecting data, evaluating data, and \\nsummarizing it into a mathematical form. Initially, statistics were related to the science'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 5, 'page_label': '6', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='various applications. Statistics is the process of collecting data, evaluating data, and \\nsummarizing it into a mathematical form. Initially, statistics were related to the science \\nof the state where it was used in the collection and analysis of facts and data about a \\ncountry such as its economy, population, etc. Mathematical statistics applies \\nmathematical techniques like linear algebra, differential equations, mathematical \\nanalysis, and theories of probability. \\nThere are two methods of analysing data in mathematical statistics that are used \\non a large scale: \\n \\nDescriptive Statistics: \\nThe descriptive method of statistics is used to describe the data collected and \\nsummarize the data and its properties using the measures of central tendencies and the \\nmeasures of dispersion. \\n \\nInferential Statistics: \\nThis method of statistics is used to draw conclusions from the data. Inferential \\nstatistics requires statistical tests performed on samples, and it draws conclusions by'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 5, 'page_label': '6', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Inferential Statistics: \\nThis method of statistics is used to draw conclusions from the data. Inferential \\nstatistics requires statistical tests performed on samples, and it draws conclusions by \\nidentifying the differences between the 2 groups. Tests calculate the p-value that is \\ncompared with the probability of chance(α) = 0.05. If the p-value is less than α, then it \\nis concluded that the p-value is statistically significant. \\n \\nDescriptive Statistics Inferential Statistics \\nDescriptive statistics are used to quantify \\nthe characteristics of the data. \\nInferential statistics are used to make \\nconclusions about the population by using \\nanalytical tools on the sample data \\nMeasures of central tendency and \\nmeasures of dispersion are the important \\ntools used. \\nHypothesis testing and regression analysis \\nare the analytical tools used. \\nIt is used to describe the characteristics of \\na known sample or population. \\nIt is used to make inferences about an \\nunknown population'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 5, 'page_label': '6', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='are the analytical tools used. \\nIt is used to describe the characteristics of \\na known sample or population. \\nIt is used to make inferences about an \\nunknown population \\nMeasures of descriptive statistics are \\nvariance, range, mean, median, etc. \\nMeasures of inferential statistics are t-test, \\nz test, linear regression, etc.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 6, 'page_label': '7', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 7 \\n \\nSH                                                       Statistics for Data Science \\n \\nDescriptive Statistics:  \\nThe study of numerical and graphical ways to describe and display your data is \\ncalled descriptive statistics. It describes the data and helps us understand the features of \\nthe data by summarizing the given sample set or population of data. In descriptive \\nstatistics, we usually take the sample into account. \\nStatisticians use graphical representation of data to get a clear picture of the data. \\nBusiness trends can be analysed easily with these representations. visual representation \\nis more effective than presenting huge numbers. \\nWe can describe these data in various dimensions. Various dimensions of \\ndescribing data are \\n \\n1. Central Tendency of Data \\n2. Dispersion of Data \\n3. Shape of the Data \\n \\n1. Central Tendency of Data: \\nThis is the centre of the distribution of data. It describes the location of data and \\nconcentrates where the data is located.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 6, 'page_label': '7', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='3. Shape of the Data \\n \\n1. Central Tendency of Data: \\nThis is the centre of the distribution of data. It describes the location of data and \\nconcentrates where the data is located. \\nThe three most widely used measures of the “centre” of the data are \\n \\n1.1 Mean \\nThe “Mean” is the average of the data. \\nAverage can be identified by summing up all the numbers and then dividing \\nthem by the number of observations. \\nMean = X1 + X2 + X3 +… + Xn / n \\nExample: Data – 10,20,30,40,50 and Number of observations = 5 \\nMean = [ 10+20+30+40+50] / 5 \\nMean = 30 \\nOutliers influence the central tendency of the data. \\n \\n1.2 Median \\nMedian is the 50%th percentile of the data. It is exactly the centre point of the \\ndata. \\nMedian can be identified by ordering the data and splits the data into two equal \\nparts and find the number. It is the best way to find the centre of the data. \\nBecause the central tendency of the data is not affected by outliers. Outliers don’t \\ninfluence the data.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 6, 'page_label': '7', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='parts and find the number. It is the best way to find the centre of the data. \\nBecause the central tendency of the data is not affected by outliers. Outliers don’t \\ninfluence the data. \\nExample: Odd number of Data – 10,20,30,40,50 \\nMedian is 30. \\nEven number of data – 10,20,30,40,50,60 \\nFind the middle 2 data and take the mean of those two values. \\nHere 30 and 40 are middle values. \\n= 30+40 / 2  \\n= 35.  Median is 35'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 7, 'page_label': '8', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 8 \\n \\nSH                                                       Statistics for Data Science \\n \\n1.3 Mode \\nMode is frequently occurring data or elements. \\nIf an element occurs the highest number of times, it is the mode of that data. If no \\nnumber in the data is repeated, then there is no mode for that data. There can be more \\nthan one mode in a dataset if two values have the same frequency and also the highest \\nfrequency. \\n \\nOutliers don’t influence the data. \\nThe mode can be calculated for both quantitative and qualitative data. \\nExample: Data – 1,3,4,6,7,3,3,5,10, 3 \\nMode is 3 \\nbecause 3 has the highest frequency (4 times) \\n \\n2. Dispersion of Data: \\n \\n  \\nThe dispersion is the “Spread of the data”. It measures how far the data is spread. \\nIn most of the dataset, the data values are closely located near the mean. On some other \\ndataset, the values are widely spread out of the mean. These dispersions of data can be \\nmeasured by  \\n \\n2.1 Inter Quartile Range (IQR)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 7, 'page_label': '8', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='dataset, the values are widely spread out of the mean. These dispersions of data can be \\nmeasured by  \\n \\n2.1 Inter Quartile Range (IQR) \\nQuartiles are special percentiles. \\n1st Quartile Q1 is the same as the 25th percentile. \\n2nd Quartile Q2 is the same as 50th percentile. \\n3rd Quartile Q3 is same as 75th percentile'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 8, 'page_label': '9', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 9 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nSteps to find quartile and percentile: \\n–The data should sort and ordered from the smallest to the largest. \\n–For Quartiles, ordered data is divided into 4 equal parts. \\n–For Percentiles, ordered data is divided into 100 equal parts. \\nInter Quartile Range is the difference between the third quartile(Q3) and the first \\nQuartile (Q1) \\nIQR = Q3- Q1 \\nInter Quartile range \\nIt is the spread of the middle half (50%) of the data \\n \\n2.2 Range \\nThe range is the difference between the largest and the smallest value in the data. \\nMax – Min = Range \\n \\n2.3 Standard Deviation \\nThe most common measure of spread is the standard deviation. \\nThe Standard deviation is the measure of how far the data deviates from the \\nmean value. \\nThe standard deviation formula varies for population and sample. Both formulas \\nare similar, but not the same. \\n• Symbol used for Sample Standard Deviation – “s” (lowercase)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 8, 'page_label': '9', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='mean value. \\nThe standard deviation formula varies for population and sample. Both formulas \\nare similar, but not the same. \\n• Symbol used for Sample Standard Deviation – “s” (lowercase) \\n• Symbol used for Population Standard Deviation – “σ” (sigma, lower case) \\n \\nSteps to find Standard deviation: \\nIf x is a number, then the difference “x – mean” is its deviation. The deviations \\nare used to calculate the standard deviation. \\nSample Standard Deviation, s = Square root of sample variance  \\nSample Standard Deviation, s = Square root of [Σ (x − x ¯ )2/ n-1]   where x ¯ is \\naverage and n is no. of samples'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 9, 'page_label': '10', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 10 \\n \\nSH                                                       Statistics for Data Science \\n \\n Standard Deviation for sample \\n \\nPopulation Standard Deviation, σ = Square root of population variance \\nPopulation Standard Deviation, σ = Square root of [ Σ (x − μ)2 / N] where μ is \\nMean and N is no. of population. \\n \\n The standard deviation for population \\n \\nThe standard deviation is always positive or zero. It will be large when the data \\nvalues are spread out from the mean. \\n \\n2.4 Variance \\nThe variance is a measure of variability. It is the average squared deviation from \\nthe mean. \\nThe symbol σ2 represents the population variance and the symbol for s2 \\nrepresents sample variance.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 10, 'page_label': '11', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 11 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\n3. Shape of the Data: \\nThe shape describes the type of the graph. \\nThe shape of the data is important because making a decision about the \\nprobability of data is based on its shape. \\nThe shape of the data can be measured by two methodologies. \\n \\n3.1 Symmetric: \\nIn the symmetric shape of the graph, the data is distributed the same on both \\nsides. \\nIn symmetric data, the mean and median are located close together. \\n \\n The curve formed by this symmetric graph is called a normal curve. \\n \\n3.2 Skewness: \\nSkewness is the measure of the asymmetry of the distribution of data. \\nThe data is not symmetrical (i.e.) it is skewed towards one side. \\nSkewness is classified into two types.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 11, 'page_label': '12', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 12 \\n \\nSH                                                       Statistics for Data Science \\n \\n1. Positively skewed \\nIn a Positively skewed distribution, the data values are clustered around the left \\nside of the distribution and the right side is longer. \\nThe mean and median will be greater than the mode in the positive skew. \\n \\n2. Negatively skewed \\nIn a Negatively skewed distribution, the data values are clustered around the \\nright side of the distribution and the left side is longer. \\nThe mean and median will be less than the mode in the negative skew. \\n \\n  \\n3.3 Kurtosis \\nKurtosis is the measure of describing the distribution of data. \\nThis data is distributed in different ways. They are, \\n \\n1. Platykurtic \\nThe platykurtic shows a distribution with flat tails. Here the data is distributed \\nflatly. The flat tails indicated the small outliers in the distribution. \\n \\n2. Mesokurtic \\nIn Mesokurtic, the data is widely distributed. It is normally distributed and it also'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 11, 'page_label': '12', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='flatly. The flat tails indicated the small outliers in the distribution. \\n \\n2. Mesokurtic \\nIn Mesokurtic, the data is widely distributed. It is normally distributed and it also \\nmatches normal distribution. \\n \\n 3. Leptokurtic \\nIn leptokurtic, the data is very closely distributed. The height of the peak is \\ngreater than width of the peak.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 12, 'page_label': '13', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 13 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nScatter Plot: \\nA scatter plot uses dots to represent values for two different numeric variables. \\nThe position of each dot on the horizontal and vertical axis indicates values for an \\nindividual data point. Scatter plots are used to observe relationships between variables.  \\nScatter plots’ primary uses are to observe and show relationships between two \\nnumeric variables. The dots in a scatter plot not only report the values of individual \\ndata points, but also patterns when the data are taken as a whole. \\n \\nScatter plot Correlation: \\nWe know that the correlation is a statistical measure of the relationship between \\nthe two variables’ relative movements. If the variables are correlated, the points will \\nfall along a line or curve. The better the correlation, the closer the points will touch the \\nline. This cause examination tool is considered as one of the seven essential quality'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 12, 'page_label': '13', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='fall along a line or curve. The better the correlation, the closer the points will touch the \\nline. This cause examination tool is considered as one of the seven essential quality \\ntools. \\n \\nTypes of correlation: \\nThe scatter plot explains the correlation between two attributes or variables. It \\nrepresents how closely the two variables are connected. There can be three such \\nsituations to see the relation between the two variables – \\n \\nPositive Correlation: \\nWhen the points in the graph are rising, moving from left to right, then the scatter \\nplot shows a positive correlation. It means the values of one variable are increasing \\nwith respect to another. \\n \\nNegative Correlation: \\nWhen the points in the scatter graph fall while moving left to right, then it is \\ncalled a negative correlation. It means the values of one variable are decreasing with \\nrespect to another.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 13, 'page_label': '14', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 14 \\n \\nSH                                                       Statistics for Data Science \\n \\nNo Correlation: \\nWhen the points are scattered all over the graph and it is difficult to conclude \\nwhether the values are increasing or decreasing, then there is no correlation between the \\nvariables.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 14, 'page_label': '15', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 15 \\n \\nSH                                                       Statistics for Data Science \\n \\nInferential Statistics: \\nInferential statistics is a branch of statistics that makes the use of various \\nanalytical tools to draw inferences about the population data from sample data. Apart \\nfrom inferential statistics, descriptive statistics forms another branch of statistics. \\nInferential statistics help to draw conclusions about the population while descriptive \\nstatistics summarizes the features of the data set. \\nThere are two main types of inferential statistics - hypothesis testing and \\nregression analysis. The samples chosen in inferential statistics need to be \\nrepresentative of the entire population. In this article, we will learn more about \\ninferential statistics, its types, examples, and see the important formulas. \\n \\nWhat are Inferential Statistics? \\nInferential statistics helps to develop a good understanding of the population data'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 14, 'page_label': '15', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='inferential statistics, its types, examples, and see the important formulas. \\n \\nWhat are Inferential Statistics? \\nInferential statistics helps to develop a good understanding of the population data \\nby analysing the samples obtained from it. It helps in making generalizations about the \\npopulation by using various analytical tests and tools. In order to pick out random \\nsamples that will represent the population accurately many sampling techniques are \\nused. Some of the important methods are simple random sampling, stratified sampling, \\ncluster sampling, and systematic sampling techniques. \\n \\nInferential Statistics Definition: \\nInferential statistics can be defined as a field of statistics that uses analytical tools \\nfor drawing conclusions about a population by examining random samples. The goal of \\ninferential statistics is to make generalizations about a population. In inferential \\nstatistics, a statistic is taken from the sample data (e.g., the sample mean) that used to'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 14, 'page_label': '15', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='inferential statistics is to make generalizations about a population. In inferential \\nstatistics, a statistic is taken from the sample data (e.g., the sample mean) that used to \\nmake inferences about the population parameter (e.g., the population mean). \\n \\nInferential Statistics Examples: \\nInferential statistics is very useful and cost-effective as it can make inferences \\nabout the population without collecting the complete data. Some inferential statistics \\nexamples are given below: \\n• Suppose the mean marks of 100 students in a particular country are known. \\nUsing this sample information, the mean marks of students in the country can be \\napproximated using inferential statistics. \\n• Suppose a coach wants to find out how many average cartwheels sophomores at \\nhis college can do without stopping. A sample of a few students will be asked to \\nperform cartwheels and the average will be calculated. Inferential statistics will use this'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 14, 'page_label': '15', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='his college can do without stopping. A sample of a few students will be asked to \\nperform cartwheels and the average will be calculated. Inferential statistics will use this \\ndata to make a conclusion regarding how many cartwheel sophomores can perform on \\naverage. \\n \\nWhy do we need Inferential Statistics? \\nIn contrast to Descriptive Statistics, rather than having access to the whole \\npopulation, we often have a limited amount of data. \\nIn such cases, Inferential Statistics come into action. For example, we might be \\ninterested in finding the average of the entire school’s exam marks. It is not reasonable \\nbecause we might find it impracticable to get the data we need. So, rather than getting'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 15, 'page_label': '16', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 16 \\n \\nSH                                                       Statistics for Data Science \\n \\nthe entire school’s exam marks, we measure a smaller sample of students (for example, \\na sample of 50 students). This sample of 50 students will now describe the complete \\npopulation of all students of that school. \\nSimply put, Inferential Statistics make predictions about a population based on a \\nsample of data taken from that population. \\n \\nThe technique of Inferential Statistics involves the following steps: \\n\\uf0b7 First, take some samples and try to find one that represents the entire \\npopulation accurately. \\n\\uf0b7 Next, test the sample and use it to draw generalizations about the whole \\npopulation. \\n \\nTypes of Inferential Statistics: \\n1. Estimating parameters:  \\nWe take a statistic from the collected data, such as the standard deviation, and \\nuse it to define a more general parameter, such as the standard deviation of the \\ncomplete population. \\n \\n2. Hypothesis testing:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 15, 'page_label': '16', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='use it to define a more general parameter, such as the standard deviation of the \\ncomplete population. \\n \\n2. Hypothesis testing:  \\nVery beneficial when we are looking to gather data on something that can only \\nbe given to a very confined population, such as a new drug. If we want to know \\nwhether this drug will work for all patients (“complete population”), we can use the \\ndata collected to predict this (often by calculating a z-score). \\n \\n3. Confidence Interval: \\nThe confidence interval is the range of values that you expect your estimate to \\nfall between a certain percentage of the time if you run your experiment again or re-\\nsample the population in the same way. \\n \\nThe confidence level is the percentage of times you expect to reproduce an \\nestimate between the upper and lower bounds of the confidence interval, and is set by \\nthe alpha value. \\n \\nConfidence interval for the mean of normally-distributed data:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 15, 'page_label': '16', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='estimate between the upper and lower bounds of the confidence interval, and is set by \\nthe alpha value. \\n \\nConfidence interval for the mean of normally-distributed data: \\nNormally-distributed data forms a bell shape when plotted on a graph, with the \\nsample mean in the middle and the rest of the data distributed fairly evenly on either \\nside of the mean. \\nThe confidence interval for data which follows a standard normal distribution is:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 16, 'page_label': '17', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 17 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nWhere: \\n\\uf0b7 CI = the confidence interval \\n\\uf0b7 X̄  = the population mean \\n\\uf0b7 Z = the critical value of the z-distribution \\n\\uf0b7 σ = the population standard deviation \\n\\uf0b7 √n = the square root of the population size \\n \\nWhat exactly is a confidence interval? \\nA confidence interval is the mean of your estimate plus and minus the variation \\nin that estimate. This is the range of values you expect your estimate to fall between if \\nyou redo your test, within a certain level of confidence. \\n \\nConfidence, in statistics, is another way to describe probability. For example, if \\nyou construct a confidence interval with a 95% confidence level, you are confident that \\n95 out of 100 times the estimate will fall between the upper and lower values specified \\nby the confidence interval. \\n \\nYour desired confidence level is usually one minus the alpha (α) value you used \\nin your statistical test'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 16, 'page_label': '17', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='by the confidence interval. \\n \\nYour desired confidence level is usually one minus the alpha (α) value you used \\nin your statistical test \\nConfidence level = 1 – α \\nSo, if you use an alpha value of p < 0.05 for statistical significance, then your \\nconfidence level would be 1 − 0.05 = 0.95, or 95%. \\n \\nWhen do you use confidence intervals? \\nYou can calculate confidence intervals for many kinds of statistical estimates, \\nincluding: \\n \\n\\uf0b7 Proportions \\n\\uf0b7 Population means \\n\\uf0b7 Differences between population means or proportions \\n\\uf0b7 Estimates of variation among groups \\nThese are all point estimates, and don’t give any information about the variation \\naround the number. Confidence intervals are useful for communicating the variation \\naround a point estimate.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 17, 'page_label': '18', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 18 \\n \\nSH                                                       Statistics for Data Science \\n \\nExample: Variation around an estimate \\nYou survey 100 Brits and 100 Americans about their television-watching habits, \\nand find that both groups watch an average of 35 hours of television per week. \\n \\nHowever, the British people surveyed had a wide variation in the number of \\nhours watched, while the Americans all watched similar amounts. \\n \\nEven though both groups have the same point estimate (average number of hours \\nwatched), the British estimate will have a wider confidence interval than the American \\nestimate because there is more variation in the data. \\n \\nFinding the standard deviation: \\nMost statistical software will have a built-in function to calculate your standard \\ndeviation, but to find it by hand you can first find your sample variance, then take the \\nsquare root to get the standard deviation. \\n \\n1. Find the sample variance'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 17, 'page_label': '18', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='deviation, but to find it by hand you can first find your sample variance, then take the \\nsquare root to get the standard deviation. \\n \\n1. Find the sample variance \\nSample variance is defined as the sum of squared differences from the mean, also \\nknown as the mean-squared-error (MSE): \\n \\n  \\nTo find the MSE, subtract your sample mean from each value in the dataset, square \\nthe resulting number, and divide that number by n − 1 \\nThen add up all of these numbers to get your total sample variance (s2). For larger \\nsample sets, it’s easiest to do this in Excel. \\n \\n2. Find the standard deviation. \\nThe standard deviation of your estimate (s) is equal to the square root of the sample \\nvariance/sample error (s2):'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 18, 'page_label': '19', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 19 \\n \\nSH                                                       Statistics for Data Science \\n \\nExample: \\nIn the television-watching survey, the variance in the GB estimate is 100, while the \\nvariance in the USA estimate is 25. Taking the square root of the variance gives us a \\nsample standard deviation (s) of: \\n\\uf0b7 10 for the GB estimate. \\n\\uf0b7 5 for the USA estimate. \\n \\nCentral Limit Theorem: \\nThe central limit theorem is the basis for how normal distributions work in \\nstatistics. \\nIn research, to get a good idea of a population mean, ideally you’d collect data \\nfrom multiple random samples within the population. A sampling distribution of the \\nmean is the distribution of the means of these different samples. \\nThe central limit theorem shows the following: \\n\\uf0b7 Law of Large Numbers: As you increase sample size (or the number of \\nsamples), then the sample mean will approach the population mean. \\n\\uf0b7 With multiple large samples, the sampling distribution of the mean is'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 18, 'page_label': '19', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='samples), then the sample mean will approach the population mean. \\n\\uf0b7 With multiple large samples, the sampling distribution of the mean is \\nnormally distributed, even if your original variable is not normally \\ndistributed. \\nParametric statistical tests typically assume that samples come from normally \\ndistributed populations, but the central limit theorem means that this assumption isn’t \\nnecessary to meet when you have a large enough sample. \\nYou can use parametric tests for large samples from populations with any kind of \\ndistribution as long as other important assumptions are met. A sample size of 30 or \\nmore is generally considered large. \\nFor small samples, the assumption of normality is important because the \\nsampling distribution of the mean isn’t known. For accurate results, you have to be sure \\nthat the population is normally distributed before you can use parametric tests with \\nsmall samples. \\n \\nWhy Is the Central Limit Theorem Useful?'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 18, 'page_label': '19', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='that the population is normally distributed before you can use parametric tests with \\nsmall samples. \\n \\nWhy Is the Central Limit Theorem Useful? \\nThe central limit theorem is useful when analysing large data sets because it allows \\none to assume that the sampling distribution of the mean will be normally-distributed in \\nmost cases. This allows for easier statistical analysis and inference.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 19, 'page_label': '20', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 20 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nHypothesis Testing: \\nHypothesis testing is a part of statistics in which we make assumptions about the \\npopulation parameter. So, hypothesis testing mentions a proper procedure by analysing \\na random sample of the population to accept or reject the assumption. \\nHypothesis testing is the way of trying to make sense of assumptions by looking \\nat the sample data. \\n \\nType of Hypothesis: \\nThe best way to determine whether a statistical hypothesis is true would be to \\nexamine the entire population. Since that is often impractical, researchers typically \\nexamine a random sample from the population. If sample data are not consistent with \\nthe statistical hypothesis, the hypothesis is rejected. \\nThere are two types of statistical hypotheses. \\n\\uf0b7 Null Hypothesis. The null hypothesis, denoted by Ho, is usually the \\nhypothesis that sample observations result purely from chance.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 19, 'page_label': '20', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='There are two types of statistical hypotheses. \\n\\uf0b7 Null Hypothesis. The null hypothesis, denoted by Ho, is usually the \\nhypothesis that sample observations result purely from chance. \\n\\uf0b7 Alternative Hypothesis. The alternative hypothesis, denoted by H1 or Ha, is \\nthe hypothesis that sample observations are influenced by some non-random \\ncause. \\n \\nSteps of Hypothesis Testing: \\nThe process to determine whether to reject a null hypothesis or to fail to reject the \\nnull hypothesis, based on sample data is called hypothesis testing. It consists of four \\nsteps: \\n1. Define the null and alternate hypothesis \\n2. Define an analysis plan to find how to use sample data to estimate the null \\nhypothesis \\n3. Do some analysis on the sample data to create a single number called ‘test \\nstatistic’ \\n4. Understand the result by applying the decision rule to check whether the Null \\nhypothesis is true or not'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 20, 'page_label': '21', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 21 \\n \\nSH                                                       Statistics for Data Science \\n \\nIf the value of t-stat is less than the significance level we will reject the null \\nhypothesis, otherwise, we will fail to reject the null hypothesis. \\nTechnically, we never accept the null hypothesis, we say that either we fail to reject \\nor we reject the null hypothesis. \\n \\nErrors in hypothesis testing: \\nWe have explained what is hypothesis testing and the steps to do the testing. \\nNow, while performing the hypothesis testing, there might be some errors. \\n\\uf0b7 Type I error. A Type I error occurs when the researcher rejects a null \\nhypothesis when it is true. The probability of committing a Type I error is \\ncalled the significance level. This probability is also called alpha and is often \\ndenoted by α. \\n\\uf0b7 Type II error. A Type II error occurs when the researcher fails to reject a \\nfalse null hypothesis. The probability of committing a Type II error is'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 20, 'page_label': '21', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='denoted by α. \\n\\uf0b7 Type II error. A Type II error occurs when the researcher fails to reject a \\nfalse null hypothesis. The probability of committing a Type II error is \\ncalled beta and is often denoted by β. The probability of not committing a \\nType II error is called the Power of the test. \\n \\nDecision \\n------------------------------ \\nActual \\nReject the null \\nhypothesis \\nFail to reject the null \\nhypothesis \\nNull Hypothesis is True Type-1 Error Decision is correct \\nAlternate hypothesis is \\ntrue \\nDecision is correct Type-2 Error \\n \\n \\nTerms in Hypothesis testing: \\nSignificance level: \\nThe significance level is defined as the probability of the case when we reject the \\nnull hypothesis, but in actuality, it is true. For example, a 0.05 significance level \\nindicates that there is a 5% risk in assuming that there is some difference when, in \\nactuality, there is no difference. It is denoted by alpha (α). \\nThe below figure shows that the two shaded regions are equidistant from the null'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 20, 'page_label': '21', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='actuality, there is no difference. It is denoted by alpha (α). \\nThe below figure shows that the two shaded regions are equidistant from the null \\nhypothesis, each having a probability of 0.025 and a total of 0.05, which is our \\nsignificance level. The shaded region in case of a two-tailed test is called the critical \\nregion.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 21, 'page_label': '22', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 22 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nP-value: \\nThe p-value is defined as the probability of seeing a t-statistic as extreme as the \\ncalculated value if the null hypothesis value is true. A low enough p-value is the ground \\nfor rejecting the null hypothesis. We reject the null hypothesis if the p-value is less than \\nthe significance level. \\n \\nZ-test: \\nA z test is used on data that follows a normal distribution and has a sample size \\ngreater than or equal to 30. It is used to test if the means of the sample and population \\nare equal when the population variance is known. The right tailed hypothesis can be set \\nup as follows: \\n \\nNull Hypothesis:  H0: μ=μ0 \\nAlternate Hypothesis:  H1: μ>μ0 \\nDecision Criteria: If the z statistic > z critical value then rejects the null hypothesis. \\nWe find the Z-statistic of the sample means and calculate the z-score. Z-score is \\ngiven by the formula,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 21, 'page_label': '22', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='We find the Z-statistic of the sample means and calculate the z-score. Z-score is \\ngiven by the formula,  \\n  \\nZ-test is mainly used when the population mean and standard deviation are \\ngiven. \\n \\nT-test: \\nA t test is used when the data follows a student t distribution and the sample size \\nis lesser than 30. It is used to compare the sample and population mean when the \\npopulation variance is unknown. The hypothesis test for inferential statistics is given as \\nfollows:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 22, 'page_label': '23', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 23 \\n \\nSH                                                       Statistics for Data Science \\n \\n \\nNull Hypothesis:   H0: μ=μ0 \\nAlternate Hypothesis: H1: μ>μ0 \\nDecision Criteria: If the t statistic > t critical value then rejects the null hypothesis. \\nThe Sample Standard Deviation is given as: \\n \\n  \\nwhere n-1 is Bessel’s correction for estimating the population parameter. \\nAnother difference between z-scores and t-values is that t-values are dependent \\non the Degree of Freedom of a sample. Let us define what degree of freedom is for a \\nsample. \\n \\nThe Degree of Freedom: \\nIt is the number of variables that have the choice of having more than one \\narbitrary value. For example, in a sample of size 10 with a mean of 10, 9 values can be \\narbitrary, but the 10th value is forced by the sample mean. \\nPoints to note about the t-tests: \\n1. The greater the difference between the sample mean and the population mean, \\nthe greater the chance of rejecting the Null Hypothesis.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 22, 'page_label': '23', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Points to note about the t-tests: \\n1. The greater the difference between the sample mean and the population mean, \\nthe greater the chance of rejecting the Null Hypothesis. \\n2. Greater the sample size, the greater the chance of rejection of the Null \\nHypothesis. \\n \\nDifferent types of T-tests: \\n1. One Sample T-test: \\nThe one-sample t-test compares the mean of sample data to a known value. So, if \\nwe have to compare the mean of sample data to the population mean, we use the One-\\nSample T-test. \\nWe can run a one-sample T-test when we do not have the population S.D., or we \\nhave a sample of size less than 30. \\nt-statistic is given by: \\n  \\nwhere, X bar is the sample mean, μ the population mean, s the sample standard \\ndeviation, and N the sample size.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 23, 'page_label': '24', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 24 \\n \\nSH                                                       Statistics for Data Science \\n \\n2. Two sample T-test: \\nWe use a two-sample T-test when we want to evaluate whether the mean of the \\ntwo samples is different or not. In a two-sample T-test, we have another two categories: \\n \\n\\uf0b7 Independent Sample T-test: \\nIndependent sample means that the two different samples should be \\nselected from two completely different populations. In other words, we can say \\nthat one population should not be dependent on the other population. \\n \\n  \\n\\uf0b7 Paired T-test: \\nIf our samples are connected in some way, we have to use the paired t-test. \\nHere, ‘connecting’ means that the samples are connected as we are collecting \\ndata from the same group two times, e.g., blood tests of patients of a hospital \\nbefore and after medication. \\n \\n \\n  \\nChi-Square test: \\nThe Chi-square test is used in the case when we have to compare categorical \\ndata.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 23, 'page_label': '24', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='before and after medication. \\n \\n \\n  \\nChi-Square test: \\nThe Chi-square test is used in the case when we have to compare categorical \\ndata. \\nThe Chi-square test is of two types. Both use chi-square statistics and distribution \\nfor different purposes. \\n\\uf0b7 The goodness of fit: It determines if sample data of categorical variables \\nmatch with population or not. \\n\\uf0b7 Test of Independence: It compares two categorical variables to find whether \\nthey are related to each other or not.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 24, 'page_label': '25', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 25 \\n \\nSH                                                       Statistics for Data Science \\n \\nChi-square statistic is given by: \\n \\n  \\nANOVA (Analysis of variance): \\nANOVA (Analysis of Variance) is used to check if at least one of two or more \\ngroups have statistically different means. Now, the question arises — Why do we need \\nanother test for checking the difference of means between independent groups? Why \\ncan we not use multiple t-tests to check for the difference in means? \\nThe answer is simple. Multiple t-tests will have a compound effect on the error \\nrate of the result. Performing a t-test thrice will give an error rate of ~15%, which is too \\nhigh, whereas ANOVA keeps it at 5% for a 95% confidence interval. \\nTo perform an ANOVA, you must have a continuous response variable and at \\nleast one categorical factor with two or more levels. ANOVA requires data from \\napproximately normally distributed populations with equal variances between factor \\nlevels.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 24, 'page_label': '25', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='least one categorical factor with two or more levels. ANOVA requires data from \\napproximately normally distributed populations with equal variances between factor \\nlevels. \\nThere are two types of ANOVA test: \\n1. One-way ANOVA: when only 1 independent variable is considered. \\n2. Two-way ANOVA: when 2 independent variables are considered. \\n3. N-way ANOVA: when N number of independent variables are considered. \\n  \\nCorrelation Coefficient (R or r): \\nIt is used to measure the strength between two variables. It is simply the square \\nroot of the coefficient of Determination and ranges from -1 to 1 where 0 represents no \\ncorrelation, and 1 represents positive strong correlation while -1 represents negative \\nstrong correlation.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 25, 'page_label': '26', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 26 \\n \\nSH                                                       Statistics for Data Science \\n \\nDifference between Z-Test and T-Test: \\n \\nBasis Z Test T-Test \\nBasic \\nDefinition \\nZ-test is a kind of hypothesis test which \\nascertains if the averages of the 2 \\ndatasets are different from each other \\nwhen standard deviation or variance is \\ngiven. \\nThe t-test can be referred to as a kind of \\nparametric test that is applied to an \\nidentity, how the averages of 2 sets of data \\ndiffer from each other when the standard \\ndeviation or variance is not given. \\nPopulation \\nVariance \\nThe Population variance or standard \\ndeviation is known here. \\nThe Population variance or standard \\ndeviation is unknown here. \\nSample Size The Sample size is large. Here the Sample Size is small. \\nKey \\nAssumptions \\nAll data points are independent. \\nNormal Distribution for Z, with an \\naverage zero and variance = 1. \\nAll data points are not dependent. \\nSample values are to be recorded and \\ntaken accurately.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 25, 'page_label': '26', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='All data points are independent. \\nNormal Distribution for Z, with an \\naverage zero and variance = 1. \\nAll data points are not dependent. \\nSample values are to be recorded and \\ntaken accurately. \\nBased upon  \\n(a type of \\ndistribution) \\nBased on Normal distribution. Based on Student-t distribution. \\n \\nDifference between One Tailed Test and Two Tailed Test: \\n \\nBASIS OF \\nCOMPARISON ONE-TAILED TEST TWO-TAILED TEST \\nMeaning A statistical hypothesis test in which \\nalternative hypothesis has only one end, \\nis known as one tailed test. \\nA significance test in which \\nalternative hypothesis has two ends, \\nis called two-tailed test. \\nHypothesis Directional Non-directional \\nRegion of rejection Either left or right Both left and right \\nDetermines If there is a relationship between \\nvariables in single direction. \\nIf there is a relationship between \\nvariables in either direction. \\nResult Greater or less than certain value. Greater or less than certain range of \\nvalues. \\nSign in alternative'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 25, 'page_label': '26', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='If there is a relationship between \\nvariables in either direction. \\nResult Greater or less than certain value. Greater or less than certain range of \\nvalues. \\nSign in alternative \\nhypothesis \\n> or < ≠'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 26, 'page_label': '27', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 27 \\n \\nSH                                                       Statistics for Data Science \\n \\nSample Vs Population: \\nSample: \\nA sample is the specific group that you will collect data from. The size of the \\nsample is always less than the total size of the population. \\nYou should calculate the sample standard deviation when the dataset you’re \\nworking with represents a sample taken from a larger population of interest. The \\nformula to calculate a sample standard deviation, denoted as s. \\nWhatever statistical measures I can calculate Here we called them as \\n“Descriptive statistics.” \\n \\nPopulation: \\nA population is the entire group that you want to draw conclusions about. \\nYou should calculate the population standard deviation when the dataset you’re \\nworking with represents an entire population, i.e., every value that you’re interested in. \\nThe formula to calculate a population standard deviation, denoted as σ.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 26, 'page_label': '27', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='working with represents an entire population, i.e., every value that you’re interested in. \\nThe formula to calculate a population standard deviation, denoted as σ. \\nWhatever the sample data, I have I will apply some additional theory and I will \\nestimate on population. ‘Inferential statistics” \\n \\n  \\nΣ: A symbol that means “sum” \\nxi: The ith value in a dataset \\nx̄ : The sample mean \\nn: The sample size \\nΣ: A symbol that means “sum” \\nxi: The ith value in a dataset \\nμ: The population mean \\nN: The population size \\n \\nBrute Force method:  \\nWhen we try to get all the data and compute it to make a statement, i.e., when we \\nreach out to the whole population. But it is difficult to compute and get each and every \\nindividual, that why we are getting into Sample.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 27, 'page_label': '28', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 28 \\n \\nSH                                                       Statistics for Data Science \\n \\nReasons for sampling: \\n\\uf0b7 Necessity:  \\nSometimes it’s simply not possible to study the whole population due to its \\nsize or inaccessibility. \\n \\n\\uf0b7 Practicality: \\nIt’s easier and more efficient to collect data from a sample. \\n \\n\\uf0b7 Cost-effectiveness:  \\nThere are fewer participant, laboratory, equipment, and researcher costs \\ninvolved. \\n \\n\\uf0b7 Manageability: \\nStoring and running statistical analyses on smaller datasets is easier and \\nreliable. \\n \\nPractice Problem 1: Height \\nSuppose a gym teacher wants to summarize the mean and standard deviation of \\nheights of students in his class. \\nWhen calculating the standard deviation of height, should he use the population \\nor sample standard deviation formula? \\nAnswer: He should use the population standard deviation because he is only \\ninterested in the height of students in this one particular class. \\n \\nPractice Problem 2: Manufacturing'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 27, 'page_label': '28', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Answer: He should use the population standard deviation because he is only \\ninterested in the height of students in this one particular class. \\n \\nPractice Problem 2: Manufacturing \\nSuppose an inspector wants to summarize the mean and standard deviation of the \\nweight of tires produced at a certain factory. He decides to collect a simple random \\nsample of 40 tires from the factory and weighs each of them. \\nWhen calculating the standard deviation of weights, should he use the population \\nor sample standard deviation formula? \\nAnswer: He should use the sample standard deviation because he is interested \\nin the weights of all tires produced at this factory, not just the weights of the tires in his \\nsample. \\n \\nPractice Problem 3: Biology \\nSuppose a biologist wants to summarize the mean and standard deviation of the \\nweight of a particular species of turtles. She decides to go out and collect a simple \\nrandom sample of 20 turtles from the population.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 27, 'page_label': '28', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='weight of a particular species of turtles. She decides to go out and collect a simple \\nrandom sample of 20 turtles from the population. \\nWhen calculating the standard deviation of weights, should she use the \\npopulation or sample standard deviation formula? \\nAnswer: She should use the sample standard deviation because she is \\ninterested in the weights of the entire population of turtles, not just the weights of the \\nturtles in her sample.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 28, 'page_label': '29', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 29 \\n \\nSH                                                       Statistics for Data Science \\n \\nPopulation and Sampling methods: \\nThe population contains all the data points from a set of data, while a sample \\nconsists of few observations selected from the population. The sample from the \\npopulation should be selected such that it has all the properties that a population has. \\nPopulation’s measurable properties such as mean, standard deviation, etc., are \\ncalled parameters, while Sample’s measurable property is known as a statistic. \\nTo draw valid conclusions from your results, you have to carefully decide how you \\nwill select a sample that is representative of the group as a whole. There are two types \\nof sampling methods: \\n \\n\\uf0b7 Probability sampling involves random selection, allowing you to make strong \\nstatistical inferences about the whole group. \\n\\uf0b7 Non-probability sampling involves non-random selection based on convenience'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 28, 'page_label': '29', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='statistical inferences about the whole group. \\n\\uf0b7 Non-probability sampling involves non-random selection based on convenience \\nor other criteria, allowing you to easily collect data. \\n \\nProbability Sampling: (Unbiased Sample) \\nProbability sampling means that every member of the population has a chance of \\nbeing selected. It is mainly used in quantitative research. If you want to produce results \\nthat are representative of the whole population, probability sampling techniques are the \\nmost valid choice. \\nThere are four main types of probability sample. \\n \\n  \\n1. Simple random sampling: \\nIn a simple random sample, every member of the population has an equal chance \\nof being selected. Your sampling frame should include the whole population. \\nTo conduct this type of sampling, you can use tools like random number \\ngenerators or other techniques that are based entirely on chance.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 29, 'page_label': '30', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 30 \\n \\nSH                                                       Statistics for Data Science \\n \\nExample: You want to select a simple random sample of 100 employees of \\nCompany X. You assign a number to every employee in the company database from 1 \\nto 1000, and use a random number generator to select 100 numbers. \\n \\n2. Systematic sampling: \\nSystematic sampling is similar to simple random sampling, but it is usually \\nslightly easier to conduct. Every member of the population is listed with a number, but \\ninstead of randomly generating numbers, individuals are chosen at regular intervals. \\nExample: All employees of the company are listed in alphabetical order. From the \\nfirst 10 numbers, you randomly select a starting point: number 6. From number 6 \\nonwards, every 10th person on the list is selected (6, 16, 26, 36, and so on), and you \\nend up with a sample of 100 people. \\nIf you use this technique, it is important to make sure that there is no hidden'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 29, 'page_label': '30', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='end up with a sample of 100 people. \\nIf you use this technique, it is important to make sure that there is no hidden \\npattern in the list that might skew the sample. For example, if the HR database groups \\nemployees by team, and team members are listed in order of seniority, there is a risk \\nthat your interval might skip over people in junior roles, resulting in a sample that is \\nskewed towards senior employees. \\n \\n3. Stratified sampling: \\nStratified sampling involves dividing the population into subpopulations that may \\ndiffer in important ways. It allows you draw more precise conclusions by ensuring that \\nevery subgroup is properly represented in the sample. \\nTo use this sampling method, you divide the population into subgroups (called \\nstrata) based on the relevant characteristic (e.g., gender, age range, income bracket, job \\nrole). \\nBased on the overall proportions of the population, you calculate how many'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 29, 'page_label': '30', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='strata) based on the relevant characteristic (e.g., gender, age range, income bracket, job \\nrole). \\nBased on the overall proportions of the population, you calculate how many \\npeople should be sampled from each subgroup. Then you use random or systematic \\nsampling to select a sample from each subgroup. \\nExample: The company has 800 female employees and 200 male employees. You \\nwant to ensure that the sample reflects the gender balance of the company, so you sort \\nthe population into two strata based on gender. Then you use random sampling on each \\ngroup, selecting 80 women and 20 men, which gives you a representative sample of \\n100 people. \\n \\n4. Cluster sampling: \\nCluster sampling also involves dividing the population into subgroups, but each \\nsubgroup should have similar characteristics to the whole sample. Instead of sampling \\nindividuals from each subgroup, you randomly select entire subgroups. \\nIf it is practically possible, you might include every individual from each sampled'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 29, 'page_label': '30', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='individuals from each subgroup, you randomly select entire subgroups. \\nIf it is practically possible, you might include every individual from each sampled \\ncluster. If the clusters themselves are large, you can also sample individuals from \\nwithin each cluster using one of the techniques above. This is called multistage \\nsampling. \\nThis method is good for dealing with large and dispersed populations, but there is \\nmore risk of error in the sample, as there could be substantial differences between'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 30, 'page_label': '31', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 31 \\n \\nSH                                                       Statistics for Data Science \\n \\nclusters. It’s difficult to guarantee that the sampled clusters are really representative of \\nthe whole population. \\nExample: The company has offices in 10 cities across the country (all with \\nroughly the same number of employees in similar roles). You don’t have the capacity to \\ntravel to every office to collect your data, so you use random sampling to select 3 \\noffices – these are your clusters. \\n \\nNon-probability sampling: (Biased Sample) \\nIn a non-probability sample, individuals are selected based on non-random \\ncriteria, and not every individual has a chance of being included. \\nThis type of sample is easier and cheaper to access, but it has a higher risk \\nof sampling bias. That means the inferences you can make about the population are \\nweaker than with probability samples, and your conclusions may be more limited. If'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 30, 'page_label': '31', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='of sampling bias. That means the inferences you can make about the population are \\nweaker than with probability samples, and your conclusions may be more limited. If \\nyou use a non-probability sample, you should still aim to make it as representative of \\nthe population as possible. \\nNon-probability sampling techniques are often used \\nin exploratory and qualitative research. In these types of research, the aim is not to test \\na hypothesis about a broad population, but to develop an initial understanding of a \\nsmall or under-researched population. \\n \\n  \\n1. Convenience sampling: \\nA convenience sample simply includes the individuals who happen to be most \\naccessible to the researcher. \\nThis is an easy and inexpensive way to gather initial data, but there is no way to \\ntell if the sample is representative of the population, so it can’t produce generalizable \\nresults. \\nExample: You are researching opinions about student support services in your'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 30, 'page_label': '31', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='tell if the sample is representative of the population, so it can’t produce generalizable \\nresults. \\nExample: You are researching opinions about student support services in your \\nuniversity, so after each of your classes, you ask your fellow students to complete'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 31, 'page_label': '32', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 32 \\n \\nSH                                                       Statistics for Data Science \\n \\na survey on the topic. This is a convenient way to gather data, but as you only surveyed \\nstudents taking the same classes as you at the same level, the sample is not \\nrepresentative of all the students at your university. \\n \\n2. Voluntary response sampling: \\nSimilar to a convenience sample, a voluntary response sample is mainly based on \\nease of access. Instead of the researcher choosing participants and directly contacting \\nthem, people volunteer themselves (e.g., by responding to a public online survey). \\nVoluntary response samples are always at least somewhat biased, as some people \\nwill inherently be more likely to volunteer than others. \\nExample: You send out the survey to all students at your university and a lot of \\nstudents decide to complete it. This can certainly give you some insight into the topic,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 31, 'page_label': '32', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Example: You send out the survey to all students at your university and a lot of \\nstudents decide to complete it. This can certainly give you some insight into the topic, \\nbut the people who responded are more likely to be those who have strong opinions \\nabout the student support services, so you can’t be sure that their opinions are \\nrepresentative of all students. \\n \\n3. Purposive sampling: \\nThis type of sampling, also known as judgement sampling, involves the \\nresearcher using their expertise to select a sample that is most useful to the purposes of \\nthe research. \\nIt is often used in qualitative research, where the researcher wants to gain \\ndetailed knowledge about a specific phenomenon rather than make statistical \\ninferences, or where the population is very small and specific. An effective purposive \\nsample must have clear criteria and rationale for inclusion. \\nExample: You want to know more about the opinions and experiences of'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 31, 'page_label': '32', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='sample must have clear criteria and rationale for inclusion. \\nExample: You want to know more about the opinions and experiences of \\ndisabled students at your university, so you purposefully select a number of students \\nwith different support needs in order to gather a varied range of data on their \\nexperiences with student services. \\n \\n4. Snowball sampling \\nIf the population is hard to access, snowball sampling can be used to recruit \\nparticipants via other participants. The number of people you have access to \\n“snowballs” as you get in contact with more people. \\nExample: You are researching experiences of homelessness in your city. Since \\nthere is no list of all homeless people in the city, probability sampling isn’t possible. \\nYou meet one person who agrees to participate in the research, and she puts you in \\ncontact with other homeless people that she knows in the area.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 32, 'page_label': '33', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 33 \\n \\nSH                                                       Statistics for Data Science \\n \\nRandom Variable and Probability Distributions: \\nProbability is defined as the likeliness of something to occur or happen \\nand probability distributions are functions that give the relation between all the \\noutcomes of a random variable in any random experiment and its probable values. \\nThese distribution functions are used in predicting the stock prices, weather \\nprediction. \\n \\nWhat is Random Variable? \\nSet of all possible values from a Random Experiment is called Random Variable. \\nIt is represented by X. \\nExample: Outcome of coin toss. \\n  \\nTypes of Random Variable: \\n\\uf0b7 Discrete Random Variable: \\nX is a discrete because it has a countable value between two numbers \\nExample: number of balls in a bag, number of tails in tossing coin \\n\\uf0b7 Continuous Random Variable: \\nX is a continuous because it has an infinite number of values between two values'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 32, 'page_label': '33', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Example: number of balls in a bag, number of tails in tossing coin \\n\\uf0b7 Continuous Random Variable: \\nX is a continuous because it has an infinite number of values between two values \\nExample: distance travelled, Height of students \\n  \\nWhat is Probability Distribution? \\nA Probability Distribution of a random variable is a list of all possible outcomes \\nwith corresponding probability values. \\nNote: The value of the probability always lies between 0 to 1. \\n \\n  \\nWhat is an example of Probability Distribution? \\nWhen two dice are rolled with six sided dots, let the possible probability of \\nrolling is as follows:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 33, 'page_label': '34', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 34 \\n \\nSH                                                       Statistics for Data Science \\n \\n \\n\\uf0b7 If a random variable is a discrete variable, its probability distribution is called \\ndiscrete probability distribution. \\no Example: Flipping of two coins \\no Functions that represent a discrete probability distribution is known \\nas Probability Mass Function. \\n\\uf0b7 If a random variable is a continuous variable, its probability distribution is called \\ncontinuous probability distribution. \\no Example: Measuring temperature over a period of time \\no Functions that represent a continuous probability distribution is known \\nas Probability Density Function. \\n \\nTypes of Probability Distributions:   \\nUniform Distribution: \\nProbability distribution in which all the outcome has equal probability is known \\nas Uniform Distribution. \\nExample: Perfect Random Generator \\nConsider an experiment of tossing a single coin: \\n \\n  \\nBernoulli Distribution:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 33, 'page_label': '34', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='as Uniform Distribution. \\nExample: Perfect Random Generator \\nConsider an experiment of tossing a single coin: \\n \\n  \\nBernoulli Distribution: \\nA discrete probability distribution for a random experiment that has only two \\npossible outcomes (Bernoulli trials) is known Bernoulli Distribution. \\nExample: India will win cricket world cup or not'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 34, 'page_label': '35', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 35 \\n \\nSH                                                       Statistics for Data Science \\n \\nIt has only two possible outcomes \\no Success (1) \\no Failure (0) \\nConsider an experiment of Shooting of Basketball \\n \\n  \\nBinomial Distribution: \\nA discrete probability distribution that gives only two possible outcomes in n \\nindependent trails is known as Binomial Distribution. \\nExample: Yes/No survey \\n\\uf0b7 Extension of Bernoulli Distribution \\n\\uf0b7 Represent the number of success and failure into n independent trials \\n\\uf0b7 The probability of success and failure is the same for all independent and \\nidentical trails. \\nLet’s understand the Binomial Distribution by an example, \\nConsider the experiment of Picking Balls \\nProblem Statement:  \\nLet there are 8 white balls and 2 black balls, then the probability of drawing 3 white \\nballs, if the probability of selecting white ball is 0.6.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 35, 'page_label': '36', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 36 \\n \\nSH                                                       Statistics for Data Science \\n \\nBernoulli Binomial \\nDeals with the single trial event Deals with the outcome of Multiple trials of the \\nsingle events \\nHas only two possible outcome 0 and 1 Sum of identically and independent distributed \\nBernoulli Random Variable \\n \\nPoisson Distribution: \\nA discrete probability distribution that measures the probability of a random \\nvariable over a specific period of time is known as Poisson Distribution. \\nExample: Probability of Asteroid collision over a selected year of period. \\n\\uf0b7 Used to predict probability of number of successful events. \\n\\uf0b7 Random variable X is Poisson distributed if the distribution function is given by: \\nNote: In case of Poisson Distribution Mean = Variance \\nLet’s understand the Poisson Distribution by an example, \\nConsider the experiment of Number of patients visiting in a hospital \\nProblem Statement:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 35, 'page_label': '36', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Let’s understand the Poisson Distribution by an example, \\nConsider the experiment of Number of patients visiting in a hospital \\nProblem Statement: \\nLet in a hospital patient arriving in a hospital at expected value is 6, then what is the \\nprobability of five patients will visit the hospital in that day? \\n \\n  \\nPoisson Binomial \\nNumber of trials are infinite Number of trials are fixed \\nUnlimited number of possible outcomes Only two possible outcomes (Success or \\nFailure) \\nMean = Variance Mean > Variance'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 36, 'page_label': '37', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 37 \\n \\nSH                                                       Statistics for Data Science \\n \\nNormal Distribution: \\n(Gaussian Distribution, Bell Curve, Symmetric Around Mean): \\nA continuous probability distribution, which is symmetric about its mean value \\n(i.e., data near the mean are more frequency in occurrence) is known as Normal \\nDistribution. \\nLet’s understand the Normal Distribution by an example, \\nConsider the experiment of Number of books read by students in a school \\n \\n  \\nEmpirical Rule: \\nEmpirical Rule is often called the 68 – 95 – 99.7 rule or Three Sigma Rule. It states \\nthat on a Normal Distribution: \\n\\uf0b7 68% of the data will be within one Standard Deviation of the Mean \\n\\uf0b7 95% of the data will be within two Standard Deviations of the Mean \\n\\uf0b7 99.7 of the data will be within three Standard Deviations of the Mean'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf', 'total_pages': 38, 'page': 37, 'page_label': '38', 'source_file': 'Statistic_For_Data_Science_1704226429.pdf', 'file_type': 'pdf'}, page_content='Page 38 \\n \\nSH                                                       Statistics for Data Science \\n \\nA Normal distribution curve has the following properties: \\no Symmetrical around its mean value \\no Mean = Median = Mode \\no Total area under the curve is 1 \\no Curve of the distribution is bell curve \\no The curve is symmetric, with half of the values on the left and half of the \\nvalues on the right. \\nDifference between Poisson and Normal Distribution: \\n \\nPoisson Normal \\nUse Discrete Data Use Continuous Data \\nDistribution varies on mean value Symmetric about mean value \\nMean = Variance Mean = Median = Mode \\n \\nStandard normal distribution: \\n\\uf0b7 Normal distribution with mean = 0 and standard deviation = 1.  \\n \\n  \\n \\nProbability distributions are not a Graph. \\nA graph is just a visual representation. \\n \\nThe End')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f809f",
   "metadata": {},
   "source": [
    "# Embedding And VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20253667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gen AI Course\\RAG Crash course\\rven\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f5b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gen AI Course\\RAG Crash course\\rven\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Vikrant\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2e71d27fa50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):   #hugging face model for embeddings\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd06293",
   "metadata": {},
   "source": [
    "# Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b8a3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2e71d3d9210>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fcf215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 91 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (91, 384)\n",
      "Adding 91 documents to vector store...\n",
      "Successfully added 91 documents to vector store\n",
      "Total documents in collection: 91\n"
     ]
    }
   ],
   "source": [
    "# converting the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "# generating embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# store in the vector DB\n",
    "vectorstore.add_documents(chunks,embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0b0a9",
   "metadata": {},
   "source": [
    "# Retriever Pipeline from VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e60b17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc649fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Normal distribution of the graph'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_3c6173b5_90',\n",
       "  'content': 'Page 38 \\n \\nSH                                                       Statistics for Data Science \\n \\nA Normal distribution curve has the following properties: \\no Symmetrical around its mean value \\no Mean = Median = Mode \\no Total area under the curve is 1 \\no Curve of the distribution is bell curve \\no The curve is symmetric, with half of the values on the left and half of the \\nvalues on the right. \\nDifference between Poisson and Normal Distribution: \\n \\nPoisson Normal \\nUse Discrete Data Use Continuous Data \\nDistribution varies on mean value Symmetric about mean value \\nMean = Variance Mean = Median = Mode \\n \\nStandard normal distribution: \\n\\uf0b7 Normal distribution with mean = 0 and standard deviation = 1.  \\n \\n  \\n \\nProbability distributions are not a Graph. \\nA graph is just a visual representation. \\n \\nThe End',\n",
       "  'metadata': {'page': 37,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Statistic_For_Data_Science_1704226429.pdf',\n",
       "   'creator': 'PyPDF',\n",
       "   'doc_index': 90,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'PyPDF',\n",
       "   'content_length': 809,\n",
       "   'creationdate': '',\n",
       "   'source_file': 'Statistic_For_Data_Science_1704226429.pdf',\n",
       "   'total_pages': 38,\n",
       "   'page_label': '38'},\n",
       "  'similarity_score': 0.23887383937835693,\n",
       "  'distance': 0.7611261606216431,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is Normal distribution of the graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96be3e7",
   "metadata": {},
   "source": [
    "# Integration VectorDB context pipeline With LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80656b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Simple RAG pipeline with Groq LLM model\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# using groq API key to initialize llm model\n",
    "# groq_api_key=\"May enter grop api directily here\"\n",
    "groq_api_key=os.getenv(\"GROQ2_API_KEY\")\n",
    "\n",
    "#LLM model \n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "# creating simple RAG function\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adc1b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Normal distribution of the graph?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Normal distribution of the graph is a bell curve that is symmetric about its mean value, with the mean, median, and mode being equal.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is Normal distribution of the graph?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1da3f208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'explain importence of statistics?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics is crucial as it enables us to collect, analyze, and interpret data, allowing us to make informed decisions and draw conclusions about a population based on a sample. It helps us understand patterns, trends, and relationships within data, which is essential in various fields such as business, medicine, social sciences, and more.\n"
     ]
    }
   ],
   "source": [
    "answer1=rag_simple(\"explain importence of statistics?\",rag_retriever,llm)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe8131",
   "metadata": {},
   "source": [
    "# Advance RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a13eeccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Explain me about z score?'\n",
      "Top K: 5, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: A z-score is a measure of how many standard deviations an element is from the mean. It is calculated using the formula:\n",
      "\n",
      "Z-score = (X - μ) / σ\n",
      "\n",
      "Where:\n",
      "- X is the value of the element\n",
      "- μ is the population mean\n",
      "- σ is the population standard deviation\n",
      "\n",
      "In the context of the Z-test, the z-score is used to determine if the sample mean is significantly different from the population mean.\n",
      "Sources: [{'source': 'Statistic_For_Data_Science_1704226429.pdf', 'page': 21, 'score': 0.21329039335250854, 'preview': 'Page 22 \\n \\nSH                                                       Statistics for Data Science \\n \\n  \\nP-value: \\nThe p-value is defined as the probability of seeing a t-statistic as extreme as the \\ncalculated value if the null hypothesis value is true. A low enough p-value is the ground \\nfor rejectin...'}, {'source': 'Statistic_For_Data_Science_1704226429.pdf', 'page': 21, 'score': 0.10023480653762817, 'preview': 'We find the Z-statistic of the sample means and calculate the z-score. Z-score is \\ngiven by the formula,  \\n  \\nZ-test is mainly used when the population mean and standard deviation are \\ngiven. \\n \\nT-test: \\nA t test is used when the data follows a student t distribution and the sample size \\nis lesser t...'}]\n",
      "Confidence: 0.21329039335250854\n",
      "Context Preview: Page 22 \n",
      " \n",
      "SH                                                       Statistics for Data Science \n",
      " \n",
      "  \n",
      "P-value: \n",
      "The p-value is defined as the probability of seeing a t-statistic as extreme as the \n",
      "calculated value if the null hypothesis value is true. A low enough p-value is the ground \n",
      "for rejecting the null hypothesis. We reject the null hypothesis if the p-value is less than \n",
      "the significance l\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Explain me about z score?\", rag_retriever, llm, top_k=5, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:400])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
